{'role': 'assistant', 'content': '# Building AI Development Interface with Claude Integration\n\nPerfect choice! Let\'s build a flexible AI development interface that supports both OpenAI and Claude. This will be a powerful tool for development assistance.\n\n## Implementation Plan\n\nI\'ll create a multi-provider AI system with:\n1. Abstract AI Provider Interface\n2. Claude (Anthropic) Provider\n3. Refactored OpenAI Provider\n4. AI Development Chat Service\n5. Backend API for development use\n\nLet\'s start!\n\n---\n\n## Step 1: Create AI Provider Interface\n\n<function_calls>\n<create-file file_path="medical-analysis-platform/lib/types/ai-provider.ts">\n/**\n * AI Provider Interface\n * \n * Abstract interface for all AI providers (OpenAI, Claude, etc.)\n * Enables easy switching between different AI models and providers.\n */\n\nexport enum AIProvider {\n  OPENAI = \'OPENAI\',\n  CLAUDE = \'CLAUDE\',\n  GEMINI = \'GEMINI\',\n  LLAMA = \'LLAMA\'\n}\n\nexport enum AIModel {\n  // OpenAI Models\n  GPT_35_TURBO = \'gpt-3.5-turbo\',\n  GPT_4 = \'gpt-4\',\n  GPT_4_TURBO = \'gpt-4-turbo\',\n  GPT_4O = \'gpt-4o\',\n  \n  // Claude Models\n  CLAUDE_3_OPUS = \'claude-3-opus-20240229\',\n  CLAUDE_3_SONNET = \'claude-3-sonnet-20240229\',\n  CLAUDE_3_HAIKU = \'claude-3-haiku-20240307\',\n  CLAUDE_35_SONNET = \'claude-3-5-sonnet-20241022\',\n  \n  // Future: Gemini, Llama, etc.\n}\n\nexport interface AIMessage {\n  role: \'system\' | \'user\' | \'assistant\';\n  content: string;\n  name?: string;\n}\n\nexport interface AICompletionRequest {\n  messages: AIMessage[];\n  model: AIModel;\n  maxTokens?: number;\n  temperature?: number;\n  topP?: number;\n  stream?: boolean;\n  stopSequences?: string[];\n  metadata?: Record<string, any>;\n}\n\nexport interface AICompletionResponse {\n  id: string;\n  content: string;\n  model: AIModel;\n  provider: AIProvider;\n  usage: {\n    promptTokens: number;\n    completionTokens: number;\n    totalTokens: number;\n  };\n  cost: number;\n  finishReason: \'stop\' | \'length\' | \'content_filter\' | \'tool_use\';\n  metadata?: Record<string, any>;\n}\n\nexport interface AIStreamChunk {\n  content: string;\n  finishReason?: \'stop\' | \'length\' | \'content_filter\';\n}\n\nexport interface AIProviderConfig {\n  provider: AIProvider;\n  apiKey: string;\n  model: AIModel;\n  maxTokens?: number;\n  temperature?: number;\n  topP?: number;\n  baseURL?: string;\n  timeout?: number;\n}\n\nexport interface AIProviderCapabilities {\n  supportsStreaming: boolean;\n  supportsVision: boolean;\n  supportsFunctionCalling: boolean;\n  supportsSystemMessages: boolean;\n  maxContextWindow: number;\n  maxOutputTokens: number;\n}\n\nexport interface AIProviderCosts {\n  promptCostPer1M: number;\n  completionCostPer1M: number;\n  currency: string;\n}\n\n/**\n * Abstract AI Provider Interface\n * All AI providers must implement this interface\n */\nexport interface IAIProvider {\n  // Provider info\n  readonly provider: AIProvider;\n  readonly model: AIModel;\n  readonly capabilities: AIProviderCapabilities;\n  readonly costs: AIProviderCosts;\n\n  // Core methods\n  complete(request: AICompletionRequest): Promise<AICompletionResponse>;\n  stream(request: AICompletionRequest): AsyncGenerator<AIStreamChunk>;\n  \n  // Utility methods\n  countTokens(text: string): number;\n  calculateCost(promptTokens: number, completionTokens: number): number;\n  validateApiKey(): Promise<boolean>;\n  \n  // Configuration\n  updateConfig(config: Partial<AIProviderConfig>): void;\n  getConfig(): AIProviderConfig;\n}\n\n/**\n * Model capabilities by provider\n */\nexport const MODEL_CAPABILITIES: Record<AIModel, AIProviderCapabilities> = {\n  // OpenAI Models\n  [AIModel.GPT_35_TURBO]: {\n    supportsStreaming: true,\n    supportsVision: false,\n    supportsFunctionCalling: true,\n    supportsSystemMessages: true,\n    maxContextWindow: 16385,\n    maxOutputTokens: 4096\n  },\n  [AIModel.GPT_4]: {\n    supportsStreaming: true,\n    supportsVision: false,\n    supportsFunctionCalling: true,\n    supportsSystemMessages: true,\n    maxContextWindow: 8192,\n    maxOutputTokens: 4096\n  },\n  [AIModel.GPT_4_TURBO]: {\n    supportsStreaming: true,\n    supportsVision: true,\n    supportsFunctionCalling: true,\n    supportsSystemMessages: true,\n    maxContextWindow: 128000,\n    maxOutputTokens: 4096\n  },\n  [AIModel.GPT_4O]: {\n    supportsStreaming: true,\n    supportsVision: true,\n    supportsFunctionCalling: true,\n    supportsSystemMessages: true,\n    maxContextWindow: 128000,\n    maxOutputTokens: 4096\n  },\n  \n  // Claude Models\n  [AIModel.CLAUDE_3_OPUS]: {\n    supportsStreaming: true,\n    supportsVision: true,\n    supportsFunctionCalling: true,\n    supportsSystemMessages: true,\n    maxContextWindow: 200000,\n    maxOutputTokens: 4096\n  },\n  [AIModel.CLAUDE_3_SONNET]: {\n    supportsStreaming: true,\n    supportsVision: true,\n    supportsFunctionCalling: true,\n    supportsSystemMessages: true,\n    maxContextWindow: 200000,\n    maxOutputTokens: 4096\n  },\n  [AIModel.CLAUDE_3_HAIKU]: {\n    supportsStreaming: true,\n    supportsVision: true,\n    supportsFunctionCalling: true,\n    supportsSystemMessages: true,\n    maxContextWindow: 200000,\n    maxOutputTokens: 4096\n  },\n  [AIModel.CLAUDE_35_SONNET]: {\n    supportsStreaming: true,\n    supportsVision: true,\n    supportsFunctionCalling: true,\n    supportsSystemMessages: true,\n    maxContextWindow: 200000,\n    maxOutputTokens: 8192\n  }\n};\n\n/**\n * Model costs (per 1M tokens)\n */\nexport const MODEL_COSTS: Record<AIModel, AIProviderCosts> = {\n  // OpenAI Models\n  [AIModel.GPT_35_TURBO]: {\n    promptCostPer1M: 0.50,\n    completionCostPer1M: 1.50,\n    currency: \'USD\'\n  },\n  [AIModel.GPT_4]: {\n    promptCostPer1M: 30.00,\n    completionCostPer1M: 60.00,\n    currency: \'USD\'\n  },\n  [AIModel.GPT_4_TURBO]: {\n    promptCostPer1M: 10.00,\n    completionCostPer1M: 30.00,\n    currency: \'USD\'\n  },\n  [AIModel.GPT_4O]: {\n    promptCostPer1M: 5.00,\n    completionCostPer1M: 15.00,\n    currency: \'USD\'\n  },\n  \n  // Claude Models\n  [AIModel.CLAUDE_3_OPUS]: {\n    promptCostPer1M: 15.00,\n    completionCostPer1M: 75.00,\n    currency: \'USD\'\n  },\n  [AIModel.CLAUDE_3_SONNET]: {\n    promptCostPer1M: 3.00,\n    completionCostPer1M: 15.00,\n    currency: \'USD\'\n  },\n  [AIModel.CLAUDE_3_HAIKU]: {\n    promptCostPer1M: 0.25,\n    completionCostPer1M: 1.25,\n    currency: \'USD\'\n  },\n  [AIModel.CLAUDE_35_SONNET]: {\n    promptCostPer1M: 3.00,\n    completionCostPer1M: 15.00,\n    currency: \'USD\'\n  }\n};\n\n/**\n * Provider-specific error types\n */\nexport class AIProviderError extends Error {\n  constructor(\n    message: string,\n    public provider: AIProvider,\n    public statusCode?: number,\n    public originalError?: any\n  ) {\n    super(message);\n    this.name = \'AIProviderError\';\n  }\n}\n\nexport class RateLimitError extends AIProviderError {\n  constructor(provider: AIProvider, retryAfter?: number) {\n    super(`Rate limit exceeded for ${provider}`, provider, 429);\n    this.name = \'RateLimitError\';\n  }\n}\n\nexport class AuthenticationError extends AIProviderError {\n  constructor(provider: AIProvider) {\n    super(`Authentication failed for ${provider}`, provider, 401);\n    this.name = \'AuthenticationError\';\n  }\n}\n\nexport class InvalidRequestError extends AIProviderError {\n  constructor(provider: AIProvider, message: string) {\n    super(`Invalid request for ${provider}: ${message}`, provider, 400);\n    this.name = \'InvalidRequestError\';\n  }\n}\n</create-file>\n</function_calls>'}