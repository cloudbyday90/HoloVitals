{'role': 'assistant', 'content': 'Now let me create a Llama provider for local LLM support via Open WebUI:\n\n<function_calls>\n<create-file file_path="medical-analysis-platform/lib/providers/LlamaProvider.ts">\n/**\n * Llama AI Provider (via Open WebUI/Ollama)\n * \n * Implementation of IAIProvider for local Llama models.\n * Supports Llama 3.2 models (1B, 3B, 11B, 90B) via Open WebUI\'s OpenAI-compatible API.\n * \n * Benefits:\n * - FREE (self-hosted, no API costs)\n * - PRIVATE (data never leaves your infrastructure)\n * - OFFLINE (works without internet)\n * - HIPAA-COMPLIANT (complete data control)\n */\n\nimport {\n  IAIProvider,\n  AIProvider,\n  AIModel,\n  AICompletionRequest,\n  AICompletionResponse,\n  AIStreamChunk,\n  AIProviderConfig,\n  AIProviderCapabilities,\n  AIProviderCosts,\n  MODEL_CAPABILITIES,\n  MODEL_COSTS,\n  AIProviderError,\n  RateLimitError,\n  AuthenticationError,\n  InvalidRequestError\n} from \'@/lib/types/ai-provider\';\n\ninterface OpenWebUIResponse {\n  id: string;\n  object: string;\n  created: number;\n  model: string;\n  choices: Array<{\n    index: number;\n    message: {\n      role: string;\n      content: string;\n    };\n    finish_reason: string;\n  }>;\n  usage: {\n    prompt_tokens: number;\n    completion_tokens: number;\n    total_tokens: number;\n  };\n}\n\nexport class LlamaProvider implements IAIProvider {\n  private config: AIProviderConfig;\n  private baseURL: string;\n\n  readonly provider = AIProvider.LLAMA;\n  readonly model: AIModel;\n  readonly capabilities: AIProviderCapabilities;\n  readonly costs: AIProviderCosts;\n\n  constructor(config: AIProviderConfig) {\n    if (config.provider !== AIProvider.LLAMA) {\n      throw new Error(\'Invalid provider for LlamaProvider\');\n    }\n\n    this.config = config;\n    this.model = config.model;\n    this.capabilities = MODEL_CAPABILITIES[config.model];\n    this.costs = MODEL_COSTS[config.model];\n\n    // Default to local Open WebUI instance\n    this.baseURL = config.baseURL || \'http://localhost:3000/api\';\n  }\n\n  /**\n   * Complete a chat request\n   */\n  async complete(request: AICompletionRequest): Promise<AICompletionResponse> {\n    try {\n      const response = await fetch(`${this.baseURL}/chat/completions`, {\n        method: \'POST\',\n        headers: {\n          \'Content-Type\': \'application/json\',\n          \'Authorization\': `Bearer ${this.config.apiKey || \'dummy-key\'}`, // Open WebUI may not require auth\n        },\n        body: JSON.stringify({\n          model: request.model,\n          messages: request.messages,\n          max_tokens: request.maxTokens || this.config.maxTokens || 4096,\n          temperature: request.temperature ?? this.config.temperature ?? 0.7,\n          top_p: request.topP ?? this.config.topP ?? 1,\n          stop: request.stopSequences,\n          stream: false\n        })\n      });\n\n      if (!response.ok) {\n        await this.handleError(response);\n      }\n\n      const data: OpenWebUIResponse = await response.json();\n      const choice = data.choices[0];\n      const content = choice.message.content || \'\';\n      const usage = data.usage;\n\n      // Calculate cost (FREE for local models)\n      const cost = this.calculateCost(\n        usage.prompt_tokens,\n        usage.completion_tokens\n      );\n\n      return {\n        id: data.id,\n        content,\n        model: request.model,\n        provider: this.provider,\n        usage: {\n          promptTokens: usage.prompt_tokens,\n          completionTokens: usage.completion_tokens,\n          totalTokens: usage.total_tokens\n        },\n        cost,\n        finishReason: this.mapFinishReason(choice.finish_reason),\n        metadata: {\n          created: data.created,\n          object: data.object\n        }\n      };\n    } catch (error) {\n      throw this.wrapError(error);\n    }\n  }\n\n  /**\n   * Stream a chat request\n   */\n  async *stream(request: AICompletionRequest): AsyncGenerator<AIStreamChunk> {\n    try {\n      const response = await fetch(`${this.baseURL}/chat/completions`, {\n        method: \'POST\',\n        headers: {\n          \'Content-Type\': \'application/json\',\n          \'Authorization\': `Bearer ${this.config.apiKey || \'dummy-key\'}`,\n        },\n        body: JSON.stringify({\n          model: request.model,\n          messages: request.messages,\n          max_tokens: request.maxTokens || this.config.maxTokens || 4096,\n          temperature: request.temperature ?? this.config.temperature ?? 0.7,\n          top_p: request.topP ?? this.config.topP ?? 1,\n          stop: request.stopSequences,\n          stream: true\n        })\n      });\n\n      if (!response.ok) {\n        await this.handleError(response);\n      }\n\n      const reader = response.body?.getReader();\n      if (!reader) {\n        throw new Error(\'No response body\');\n      }\n\n      const decoder = new TextDecoder();\n      let buffer = \'\';\n\n      while (true) {\n        const { done, value } = await reader.read();\n        if (done) break;\n\n        buffer += decoder.decode(value, { stream: true });\n        const lines = buffer.split(\'\\n\');\n        buffer = lines.pop() || \'\';\n\n        for (const line of lines) {\n          if (line.startsWith(\'data: \')) {\n            const data = line.slice(6);\n            if (data === \'[DONE]\') continue;\n\n            try {\n              const parsed = JSON.parse(data);\n              const delta = parsed.choices[0]?.delta;\n              \n              if (delta?.content) {\n                yield {\n                  content: delta.content,\n                  finishReason: null,\n                  model: request.model,\n                  provider: this.provider\n                };\n              }\n\n              if (parsed.choices[0]?.finish_reason) {\n                yield {\n                  content: \'\',\n                  finishReason: this.mapFinishReason(parsed.choices[0].finish_reason),\n                  model: request.model,\n                  provider: this.provider\n                };\n              }\n            } catch (e) {\n              // Skip invalid JSON\n            }\n          }\n        }\n      }\n    } catch (error) {\n      throw this.wrapError(error);\n    }\n  }\n\n  /**\n   * Count tokens in text (estimated for Llama models)\n   */\n  countTokens(text: string): number {\n    // Rough estimation: ~4 characters per token for Llama models\n    // This is less accurate than tiktoken but works for local models\n    return Math.ceil(text.length / 4);\n  }\n\n  /**\n   * Calculate cost for request (FREE for local models)\n   */\n  calculateCost(promptTokens: number, completionTokens: number): number {\n    // Local models are FREE\n    return 0;\n  }\n\n  /**\n   * Validate configuration\n   */\n  validateConfig(): boolean {\n    // Check if baseURL is accessible\n    if (!this.config.baseURL && !this.baseURL) {\n      console.warn(\'No baseURL provided, using default: http://localhost:3000/api\');\n    }\n    return true;\n  }\n\n  /**\n   * Get provider information\n   */\n  getInfo(): {\n    provider: AIProvider;\n    model: AIModel;\n    capabilities: AIProviderCapabilities;\n    costs: AIProviderCosts;\n  } {\n    return {\n      provider: this.provider,\n      model: this.model,\n      capabilities: this.capabilities,\n      costs: this.costs\n    };\n  }\n\n  /**\n   * Map finish reason to standard format\n   */\n  private mapFinishReason(reason: string): \'stop\' | \'length\' | \'content_filter\' | \'error\' {\n    switch (reason) {\n      case \'stop\':\n        return \'stop\';\n      case \'length\':\n      case \'max_tokens\':\n        return \'length\';\n      case \'content_filter\':\n        return \'content_filter\';\n      default:\n        return \'stop\';\n    }\n  }\n\n  /**\n   * Handle HTTP errors\n   */\n  private async handleError(response: Response): Promise<never> {\n    const status = response.status;\n    let errorMessage = `HTTP ${status}: ${response.statusText}`;\n\n    try {\n      const errorData = await response.json();\n      errorMessage = errorData.error?.message || errorMessage;\n    } catch {\n      // Use default error message\n    }\n\n    if (status === 429) {\n      throw new RateLimitError(this.provider);\n    } else if (status === 401 || status === 403) {\n      throw new AuthenticationError(this.provider);\n    } else if (status >= 400 && status < 500) {\n      throw new InvalidRequestError(this.provider, errorMessage);\n    } else {\n      throw new AIProviderError(errorMessage, this.provider, status);\n    }\n  }\n\n  /**\n   * Wrap unknown errors\n   */\n  private wrapError(error: any): AIProviderError {\n    if (error instanceof AIProviderError) {\n      return error;\n    }\n\n    return new AIProviderError(\n      error.message || \'Unknown error occurred\',\n      this.provider,\n      undefined,\n      error\n    );\n  }\n}\n</create-file>\n</function_calls>'}