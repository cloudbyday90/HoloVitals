{'role': 'assistant', 'content': '<function_calls>\n<create-file file_path="docs/AI_ARCHITECTURE.md">\n# HoloVitals AI Architecture\n\n## Dual AI System Design\n\n### Overview\n\nHoloVitals uses a **two-tier AI architecture** to balance cost, performance, and HIPAA compliance:\n\n1. **Lightweight Chatbot** - Always available, fast responses, general queries\n2. **Heavy-Duty Analysis Engine** - HIPAA-compliant, ephemeral instances, deep analysis\n\n## Architecture Diagram\n\n```\n┌─────────────────────────────────────────────────────────────────┐\n│                         User Interface                           │\n│                      (Next.js Frontend)                          │\n└────────────────────────────┬────────────────────────────────────┘\n                             │\n                ┌────────────┴────────────┐\n                │                         │\n                ▼                         ▼\n┌───────────────────────────┐  ┌──────────────────────────────┐\n│   Lightweight Chatbot     │  │  Document Upload Handler     │\n│   (Always Running)        │  │  (API Route)                 │\n│                           │  │                              │\n│   • GPT-3.5 Turbo        │  │  • File validation           │\n│   • <2 second response   │  │  • PHI sanitization          │\n│   • General queries      │  │  • Queue for analysis        │\n│   • Navigation help      │  └──────────────┬───────────────┘\n│   • Quick answers        │                 │\n└───────────┬───────────────┘                 │\n            │                                 │\n            │ Escalates complex queries       │\n            │                                 │\n            └────────────┬────────────────────┘\n                         │\n                         ▼\n┌─────────────────────────────────────────────────────────────────┐\n│                    Analysis Queue System                         │\n│                   (Redis/PostgreSQL)                             │\n│                                                                   │\n│  • Priority queue (URGENT, HIGH, NORMAL, LOW)                   │\n│  • Task distribution                                             │\n│  • Status tracking                                               │\n│  • Result caching                                                │\n└────────────────────────────┬────────────────────────────────────┘\n                             │\n                ┌────────────┴────────────┐\n                │                         │\n                ▼                         ▼\n┌───────────────────────────┐  ┌──────────────────────────────┐\n│  Context Optimizer        │  │  Instance Provisioner        │\n│  (Pre-processing)         │  │  (Azure/AWS)                 │\n│                           │  │                              │\n│  • Analyze context size   │  │  • Spin up HIPAA instance   │\n│  • Split if needed        │  │  • Load LLM model           │\n│  • Optimize prompts       │  │  • Execute analysis         │\n│  • Select best LLM        │  │  • Return results           │\n│                           │  │  • De-provision instance    │\n└───────────┬───────────────┘  └──────────────┬───────────────┘\n            │                                 │\n            └────────────┬────────────────────┘\n                         │\n                         ▼\n┌─────────────────────────────────────────────────────────────────┐\n│              HIPAA-Compliant Analysis Engine                     │\n│              (Ephemeral Cloud Instances)                         │\n│                                                                   │\n│  Available Models (Auto-Selected):                               │\n│  • GPT-4 Turbo (128k context) - Complex analysis                │\n│  • GPT-4 (8k context) - Standard analysis                       │\n│  • Claude 3 Opus (200k context) - Large documents               │\n│  • Claude 3 Sonnet (200k context) - Cost-efficient              │\n│  • Llama 3 70B (8k context) - Open source option                │\n│                                                                   │\n│  Instance Lifecycle: 5-30 minutes                                │\n└────────────────────────────┬────────────────────────────────────┘\n                             │\n                             ▼\n┌─────────────────────────────────────────────────────────────────┐\n│                    Results Processing                            │\n│                                                                   │\n│  • PHI sanitization                                              │\n│  • Result validation                                             │\n│  • Cache storage                                                 │\n│  • User notification                                             │\n└─────────────────────────────────────────────────────────────────┘\n```\n\n## Component Details\n\n### 1. Lightweight Chatbot (Tier 1)\n\n**Purpose:** Handle 80% of user interactions with fast, cost-effective responses\n\n**Technology Stack:**\n- **Model:** OpenAI GPT-3.5 Turbo (4k context)\n- **Hosting:** Always-on API endpoint (Next.js API route)\n- **Response Time:** <2 seconds\n- **Cost:** ~$0.002 per interaction\n\n**Capabilities:**\n- General health questions\n- Medication reminders\n- Appointment scheduling\n- Navigation assistance\n- Document upload guidance\n- Quick lookups (symptoms, medications)\n- Conversational interface\n\n**Limitations:**\n- No PHI processing\n- No deep medical analysis\n- No document analysis\n- Limited context window\n\n**Implementation:**\n\n```typescript\n// services/LightweightChatbotService.ts\n\ninterface ChatbotConfig {\n  model: \'gpt-3.5-turbo\';\n  maxTokens: 500;\n  temperature: 0.7;\n  systemPrompt: string;\n}\n\ninterface ChatbotResponse {\n  message: string;\n  needsEscalation: boolean;\n  escalationReason?: string;\n  confidence: number;\n}\n\nclass LightweightChatbotService {\n  private config: ChatbotConfig = {\n    model: \'gpt-3.5-turbo\',\n    maxTokens: 500,\n    temperature: 0.7,\n    systemPrompt: `You are a helpful health assistant for HoloVitals. \n    You provide general health information, help with navigation, and answer \n    basic questions. For complex medical analysis or document review, \n    escalate to the deep analysis system. Never provide medical diagnoses \n    or treatment recommendations - always recommend consulting healthcare \n    providers for medical advice.`\n  };\n\n  async chat(\n    userId: string,\n    message: string,\n    conversationHistory: ChatMessage[]\n  ): Promise<ChatbotResponse> {\n    // Check if escalation is needed\n    const escalationCheck = this.shouldEscalate(message);\n    \n    if (escalationCheck.shouldEscalate) {\n      return {\n        message: "I\'ll connect you with our deep analysis system for a comprehensive review.",\n        needsEscalation: true,\n        escalationReason: escalationCheck.reason,\n        confidence: 1.0\n      };\n    }\n\n    // Generate response using GPT-3.5\n    const response = await this.generateResponse(message, conversationHistory);\n    \n    return {\n      message: response.content,\n      needsEscalation: false,\n      confidence: response.confidence\n    };\n  }\n\n  private shouldEscalate(message: string): {\n    shouldEscalate: boolean;\n    reason?: string;\n  } {\n    const escalationKeywords = [\n      \'analyze document\',\n      \'review my labs\',\n      \'interpret results\',\n      \'diagnosis\',\n      \'treatment plan\',\n      \'medical opinion\',\n      \'compare my results\'\n    ];\n\n    const lowerMessage = message.toLowerCase();\n    \n    for (const keyword of escalationKeywords) {\n      if (lowerMessage.includes(keyword)) {\n        return {\n          shouldEscalate: true,\n          reason: `Complex query requiring deep analysis: ${keyword}`\n        };\n      }\n    }\n\n    return { shouldEscalate: false };\n  }\n\n  private async generateResponse(\n    message: string,\n    history: ChatMessage[]\n  ): Promise<{ content: string; confidence: number }> {\n    // Implementation using OpenAI API\n    // Returns quick, general responses\n    return {\n      content: "Generated response",\n      confidence: 0.85\n    };\n  }\n}\n```\n\n### 2. Heavy-Duty Analysis Engine (Tier 2)\n\n**Purpose:** Deep medical document analysis, complex queries, comprehensive insights\n\n**Technology Stack:**\n- **Models:** GPT-4 Turbo, Claude 3 Opus/Sonnet, Llama 3 70B\n- **Hosting:** Ephemeral cloud instances (Azure/AWS)\n- **Response Time:** 5-30 minutes\n- **Cost:** $0.50-$5.00 per analysis (depending on complexity)\n\n**Capabilities:**\n- Full medical document analysis\n- Multi-document comparison\n- Trend analysis over time\n- Complex medical reasoning\n- Detailed recommendations\n- Risk assessment\n- Drug interaction analysis\n\n**HIPAA Compliance:**\n- Azure Health Data Services\n- AWS HealthLake\n- Encrypted data in transit and at rest\n- Audit logging\n- Access controls\n- PHI sanitization before caching\n\n### 3. Context Window Optimization\n\n**Problem:** Large documents may exceed LLM context windows\n\n**Solution:** Intelligent prompt splitting and parallel processing\n\n```typescript\n// services/ContextOptimizerService.ts\n\ninterface ContextAnalysis {\n  totalTokens: number;\n  contextWindow: number;\n  needsSplitting: boolean;\n  splitStrategy?: \'sequential\' | \'parallel\' | \'hierarchical\';\n  estimatedCost: number;\n  recommendedModel: string;\n}\n\ninterface PromptSplit {\n  id: string;\n  content: string;\n  tokens: number;\n  order: number;\n  dependencies: string[];\n}\n\nclass ContextOptimizerService {\n  // Model context windows\n  private modelContextWindows = {\n    \'gpt-4-turbo\': 128000,\n    \'gpt-4\': 8192,\n    \'claude-3-opus\': 200000,\n    \'claude-3-sonnet\': 200000,\n    \'llama-3-70b\': 8192\n  };\n\n  // Model costs per 1M tokens (input/output)\n  private modelCosts = {\n    \'gpt-4-turbo\': { input: 10, output: 30 },\n    \'gpt-4\': { input: 30, output: 60 },\n    \'claude-3-opus\': { input: 15, output: 75 },\n    \'claude-3-sonnet\': { input: 3, output: 15 },\n    \'llama-3-70b\': { input: 0.9, output: 0.9 } // Self-hosted estimate\n  };\n\n  async analyzeContext(\n    documents: string[],\n    analysisType: string\n  ): Promise<ContextAnalysis> {\n    // Calculate total tokens\n    const totalTokens = this.estimateTokens(documents.join(\'\\n\'));\n    \n    // Select optimal model\n    const recommendedModel = this.selectOptimalModel(totalTokens, analysisType);\n    const contextWindow = this.modelContextWindows[recommendedModel];\n    \n    // Determine if splitting is needed\n    const needsSplitting = totalTokens > (contextWindow * 0.8); // 80% threshold\n    \n    // Calculate estimated cost\n    const estimatedCost = this.calculateCost(\n      totalTokens,\n      recommendedModel,\n      needsSplitting\n    );\n\n    return {\n      totalTokens,\n      contextWindow,\n      needsSplitting,\n      splitStrategy: needsSplitting ? this.determineSplitStrategy(analysisType) : undefined,\n      estimatedCost,\n      recommendedModel\n    };\n  }\n\n  async splitPrompt(\n    content: string,\n    contextWindow: number,\n    strategy: \'sequential\' | \'parallel\' | \'hierarchical\'\n  ): Promise<PromptSplit[]> {\n    const tokens = this.estimateTokens(content);\n    const maxTokensPerSplit = Math.floor(contextWindow * 0.7); // Leave room for system prompt\n\n    if (strategy === \'sequential\') {\n      return this.splitSequentially(content, maxTokensPerSplit);\n    } else if (strategy === \'parallel\') {\n      return this.splitInParallel(content, maxTokensPerSplit);\n    } else {\n      return this.splitHierarchically(content, maxTokensPerSplit);\n    }\n  }\n\n  private selectOptimalModel(\n    tokens: number,\n    analysisType: string\n  ): string {\n    // Priority: Cost efficiency > Context window > Performance\n    \n    if (tokens < 7000) {\n      // Small documents - use most cost-efficient\n      return \'llama-3-70b\'; // or \'claude-3-sonnet\'\n    } else if (tokens < 100000) {\n      // Medium documents - balance cost and performance\n      return \'gpt-4-turbo\';\n    } else {\n      // Large documents - need large context window\n      return \'claude-3-opus\';\n    }\n  }\n\n  private determineSplitStrategy(analysisType: string): \'sequential\' | \'parallel\' | \'hierarchical\' {\n    // Sequential: For time-series data, chronological analysis\n    // Parallel: For independent sections (multiple lab reports)\n    // Hierarchical: For complex documents with subsections\n\n    if (analysisType.includes(\'timeline\') || analysisType.includes(\'trend\')) {\n      return \'sequential\';\n    } else if (analysisType.includes(\'compare\') || analysisType.includes(\'multiple\')) {\n      return \'parallel\';\n    } else {\n      return \'hierarchical\';\n    }\n  }\n\n  private splitSequentially(content: string, maxTokens: number): PromptSplit[] {\n    // Split content into sequential chunks\n    const chunks: PromptSplit[] = [];\n    let currentChunk = \'\';\n    let currentTokens = 0;\n    let order = 0;\n\n    const paragraphs = content.split(\'\\n\\n\');\n\n    for (const paragraph of paragraphs) {\n      const paragraphTokens = this.estimateTokens(paragraph);\n\n      if (currentTokens + paragraphTokens > maxTokens) {\n        // Save current chunk\n        chunks.push({\n          id: `chunk-${order}`,\n          content: currentChunk,\n          tokens: currentTokens,\n          order: order,\n          dependencies: order > 0 ? [`chunk-${order - 1}`] : []\n        });\n\n        // Start new chunk\n        currentChunk = paragraph;\n        currentTokens = paragraphTokens;\n        order++;\n      } else {\n        currentChunk += \'\\n\\n\' + paragraph;\n        currentTokens += paragraphTokens;\n      }\n    }\n\n    // Add final chunk\n    if (currentChunk) {\n      chunks.push({\n        id: `chunk-${order}`,\n        content: currentChunk,\n        tokens: currentTokens,\n        order: order,\n        dependencies: order > 0 ? [`chunk-${order - 1}`] : []\n      });\n    }\n\n    return chunks;\n  }\n\n  private splitInParallel(content: string, maxTokens: number): PromptSplit[] {\n    // Split content into independent parallel chunks\n    // Each chunk can be processed simultaneously\n    const chunks: PromptSplit[] = [];\n    const sections = this.identifySections(content);\n\n    let order = 0;\n    for (const section of sections) {\n      const sectionTokens = this.estimateTokens(section);\n\n      if (sectionTokens <= maxTokens) {\n        chunks.push({\n          id: `section-${order}`,\n          content: section,\n          tokens: sectionTokens,\n          order: order,\n          dependencies: [] // No dependencies - can run in parallel\n        });\n        order++;\n      } else {\n        // Section too large - split further\n        const subChunks = this.splitSequentially(section, maxTokens);\n        chunks.push(...subChunks.map(chunk => ({\n          ...chunk,\n          id: `section-${order}-${chunk.id}`,\n          order: order\n        })));\n        order++;\n      }\n    }\n\n    return chunks;\n  }\n\n  private splitHierarchically(content: string, maxTokens: number): PromptSplit[] {\n    // Create hierarchical structure: summary → details\n    const chunks: PromptSplit[] = [];\n\n    // First pass: Create summary\n    const summary = this.extractSummary(content);\n    chunks.push({\n      id: \'summary\',\n      content: summary,\n      tokens: this.estimateTokens(summary),\n      order: 0,\n      dependencies: []\n    });\n\n    // Second pass: Split detailed sections\n    const sections = this.identifySections(content);\n    let order = 1;\n\n    for (const section of sections) {\n      const sectionTokens = this.estimateTokens(section);\n\n      if (sectionTokens <= maxTokens) {\n        chunks.push({\n          id: `detail-${order}`,\n          content: section,\n          tokens: sectionTokens,\n          order: order,\n          dependencies: [\'summary\'] // Depends on summary\n        });\n        order++;\n      }\n    }\n\n    return chunks;\n  }\n\n  private estimateTokens(text: string): number {\n    // Rough estimation: 1 token ≈ 4 characters\n    return Math.ceil(text.length / 4);\n  }\n\n  private identifySections(content: string): string[] {\n    // Identify logical sections in document\n    // Look for headers, page breaks, etc.\n    return content.split(/\\n#{1,3}\\s+/);\n  }\n\n  private extractSummary(content: string): string {\n    // Extract or generate summary of content\n    // Take first few paragraphs or key sections\n    const paragraphs = content.split(\'\\n\\n\');\n    return paragraphs.slice(0, 3).join(\'\\n\\n\');\n  }\n\n  private calculateCost(\n    tokens: number,\n    model: string,\n    needsSplitting: boolean\n  ): number {\n    const costs = this.modelCosts[model];\n    const inputCost = (tokens / 1000000) * costs.input;\n    const outputCost = (tokens * 0.3 / 1000000) * costs.output; // Assume 30% output\n\n    if (needsSplitting) {\n      // Add overhead for splitting (multiple API calls)\n      return (inputCost + outputCost) * 1.5;\n    }\n\n    return inputCost + outputCost;\n  }\n}\n```\n\n### 4. Queue System\n\n**Purpose:** Manage analysis tasks efficiently with priority handling\n\n```typescript\n// services/AnalysisQueueService.ts\n\nenum QueuePriority {\n  URGENT = 1,    // <5 minutes\n  HIGH = 2,      // <15 minutes\n  NORMAL = 3,    // <30 minutes\n  LOW = 4        // <60 minutes\n}\n\ninterface QueueTask {\n  id: string;\n  userId: string;\n  type: \'document_analysis\' | \'complex_query\' | \'comparison\' | \'trend_analysis\';\n  priority: QueuePriority;\n  documents: string[];\n  prompt: string;\n  contextAnalysis: ContextAnalysis;\n  status: \'queued\' | \'processing\' | \'completed\' | \'failed\';\n  createdAt: Date;\n  startedAt?: Date;\n  completedAt?: Date;\n  result?: any;\n  error?: string;\n}\n\nclass AnalysisQueueService {\n  async addToQueue(task: Omit<QueueTask, \'id\' | \'status\' | \'createdAt\'>): Promise<string> {\n    const queueTask: QueueTask = {\n      ...task,\n      id: this.generateTaskId(),\n      status: \'queued\',\n      createdAt: new Date()\n    };\n\n    // Add to database queue\n    await prisma.analysisQueue.create({\n      data: queueTask\n    });\n\n    // Notify user\n    await this.notifyUser(task.userId, \'queued\', queueTask.id);\n\n    // Trigger processing\n    await this.processQueue();\n\n    return queueTask.id;\n  }\n\n  async processQueue(): Promise<void> {\n    // Get next task by priority\n    const task = await this.getNextTask();\n\n    if (!task) return;\n\n    try {\n      // Update status\n      await this.updateTaskStatus(task.id, \'processing\');\n\n      // Provision instance and execute\n      const result = await this.executeAnalysis(task);\n\n      // Update with result\n      await this.updateTaskStatus(task.id, \'completed\', result);\n\n      // Notify user\n      await this.notifyUser(task.userId, \'completed\', task.id);\n\n    } catch (error) {\n      await this.updateTaskStatus(task.id, \'failed\', null, error.message);\n      await this.notifyUser(task.userId, \'failed\', task.id);\n    }\n\n    // Process next task\n    await this.processQueue();\n  }\n\n  private async getNextTask(): Promise<QueueTask | null> {\n    return await prisma.analysisQueue.findFirst({\n      where: { status: \'queued\' },\n      orderBy: [\n        { priority: \'asc\' },\n        { createdAt: \'asc\' }\n      ]\n    });\n  }\n\n  private async executeAnalysis(task: QueueTask): Promise<any> {\n    // This will call the instance provisioner\n    const provisioner = new InstanceProvisionerService();\n    return await provisioner.executeAnalysis(task);\n  }\n\n  private generateTaskId(): string {\n    return `task-${Date.now()}-${Math.random().toString(36).substr(2, 9)}`;\n  }\n\n  private async updateTaskStatus(\n    taskId: string,\n    status: QueueTask[\'status\'],\n    result?: any,\n    error?: string\n  ): Promise<void> {\n    await prisma.analysisQueue.update({\n      where: { id: taskId },\n      data: {\n        status,\n        result,\n        error,\n        ...(status === \'processing\' && { startedAt: new Date() }),\n        ...(status === \'completed\' && { completedAt: new Date() }),\n        ...(status === \'failed\' && { completedAt: new Date() })\n      }\n    });\n  }\n\n  private async notifyUser(\n    userId: string,\n    status: string,\n    taskId: string\n  ): Promise<void> {\n    // Send notification to user\n    // Implementation depends on notification service\n  }\n}\n```\n\n### 5. Instance Provisioner (Cloud Infrastructure)\n\n**Purpose:** Spin up ephemeral HIPAA-compliant instances for analysis\n\n```typescript\n// services/InstanceProvisionerService.ts\n\ninterface CloudInstance {\n  id: string;\n  provider: \'azure\' | \'aws\';\n  region: string;\n  instanceType: string;\n  model: string;\n  status: \'provisioning\' | \'ready\' | \'executing\' | \'deprovisioning\' | \'terminated\';\n  createdAt: Date;\n  terminatedAt?: Date;\n  cost: number;\n}\n\nclass InstanceProvisionerService {\n  private azureClient: any; // Azure SDK client\n  private awsClient: any;   // AWS SDK client\n\n  async executeAnalysis(task: QueueTask): Promise<any> {\n    let instance: CloudInstance | null = null;\n\n    try {\n      // 1. Provision instance\n      instance = await this.provisionInstance(\n        task.contextAnalysis.recommendedModel\n      );\n\n      // 2. Load model\n      await this.loadModel(instance, task.contextAnalysis.recommendedModel);\n\n      // 3. Execute analysis\n      const result = await this.runAnalysis(instance, task);\n\n      // 4. Sanitize result (remove any PHI before caching)\n      const sanitizedResult = await this.sanitizeResult(result);\n\n      // 5. Return result\n      return sanitizedResult;\n\n    } finally {\n      // 6. Always deprovision instance\n      if (instance) {\n        await this.deprovisionInstance(instance);\n      }\n    }\n  }\n\n  private async provisionInstance(model: string): Promise<CloudInstance> {\n    // Determine provider based on model and availability\n    const provider = this.selectProvider(model);\n\n    if (provider === \'azure\') {\n      return await this.provisionAzureInstance(model);\n    } else {\n      return await this.provisionAWSInstance(model);\n    }\n  }\n\n  private async provisionAzureInstance(model: string): Promise<CloudInstance> {\n    // Use Azure Health Data Services\n    // Provision GPU instance for model inference\n    \n    const instanceConfig = {\n      resourceGroup: \'holovitals-hipaa\',\n      location: \'eastus\',\n      vmSize: this.getVMSize(model),\n      image: \'hipaa-compliant-ml-image\',\n      tags: {\n        purpose: \'medical-analysis\',\n        compliance: \'hipaa\',\n        model: model\n      }\n    };\n\n    // Provision VM\n    const vm = await this.azureClient.virtualMachines.createOrUpdate(\n      instanceConfig.resourceGroup,\n      `analysis-${Date.now()}`,\n      instanceConfig\n    );\n\n    return {\n      id: vm.id,\n      provider: \'azure\',\n      region: instanceConfig.location,\n      instanceType: instanceConfig.vmSize,\n      model: model,\n      status: \'provisioning\',\n      createdAt: new Date(),\n      cost: 0\n    };\n  }\n\n  private async provisionAWSInstance(model: string): Promise<CloudInstance> {\n    // Use AWS HealthLake\n    // Provision EC2 instance with GPU for model inference\n\n    const instanceConfig = {\n      ImageId: \'ami-hipaa-compliant-ml\',\n      InstanceType: this.getInstanceType(model),\n      MinCount: 1,\n      MaxCount: 1,\n      SecurityGroupIds: [\'sg-hipaa-compliant\'],\n      SubnetId: \'subnet-private-hipaa\',\n      TagSpecifications: [{\n        ResourceType: \'instance\',\n        Tags: [\n          { Key: \'Purpose\', Value: \'medical-analysis\' },\n          { Key: \'Compliance\', Value: \'hipaa\' },\n          { Key: \'Model\', Value: model }\n        ]\n      }]\n    };\n\n    // Launch instance\n    const response = await this.awsClient.runInstances(instanceConfig);\n\n    return {\n      id: response.Instances[0].InstanceId,\n      provider: \'aws\',\n      region: \'us-east-1\',\n      instanceType: instanceConfig.InstanceType,\n      model: model,\n      status: \'provisioning\',\n      createdAt: new Date(),\n      cost: 0\n    };\n  }\n\n  private async loadModel(instance: CloudInstance, model: string): Promise<void> {\n    // Load model onto instance\n    // This could involve:\n    // 1. Downloading model weights from secure storage\n    // 2. Loading into GPU memory\n    // 3. Warming up model\n    \n    // Wait for model to be ready\n    await this.waitForModelReady(instance);\n  }\n\n  private async runAnalysis(\n    instance: CloudInstance,\n    task: QueueTask\n  ): Promise<any> {\n    // Execute analysis on instance\n    \n    if (task.contextAnalysis.needsSplitting) {\n      // Run split analysis\n      return await this.runSplitAnalysis(instance, task);\n    } else {\n      // Run single analysis\n      return await this.runSingleAnalysis(instance, task);\n    }\n  }\n\n  private async runSplitAnalysis(\n    instance: CloudInstance,\n    task: QueueTask\n  ): Promise<any> {\n    const optimizer = new ContextOptimizerService();\n    \n    // Split prompt\n    const splits = await optimizer.splitPrompt(\n      task.prompt,\n      task.contextAnalysis.contextWindow,\n      task.contextAnalysis.splitStrategy!\n    );\n\n    // Process splits based on strategy\n    if (task.contextAnalysis.splitStrategy === \'parallel\') {\n      // Process in parallel\n      const results = await Promise.all(\n        splits.map(split => this.executeSplit(instance, split, task))\n      );\n      return this.mergeResults(results);\n    } else {\n      // Process sequentially\n      const results = [];\n      for (const split of splits) {\n        const result = await this.executeSplit(instance, split, task);\n        results.push(result);\n      }\n      return this.mergeResults(results);\n    }\n  }\n\n  private async runSingleAnalysis(\n    instance: CloudInstance,\n    task: QueueTask\n  ): Promise<any> {\n    // Make API call to model on instance\n    const response = await this.callModel(instance, {\n      prompt: task.prompt,\n      documents: task.documents,\n      model: instance.model\n    });\n\n    return response;\n  }\n\n  private async executeSplit(\n    instance: CloudInstance,\n    split: PromptSplit,\n    task: QueueTask\n  ): Promise<any> {\n    // Execute single split\n    return await this.callModel(instance, {\n      prompt: split.content,\n      documents: task.documents,\n      model: instance.model\n    });\n  }\n\n  private async callModel(\n    instance: CloudInstance,\n    params: any\n  ): Promise<any> {\n    // Make HTTP request to model API on instance\n    // This is provider-specific\n    \n    if (instance.provider === \'azure\') {\n      return await this.callAzureModel(instance, params);\n    } else {\n      return await this.callAWSModel(instance, params);\n    }\n  }\n\n  private async sanitizeResult(result: any): Promise<any> {\n    // Remove any PHI before caching\n    const sanitizer = new HIPAASanitizerService();\n    return await sanitizer.sanitize(result);\n  }\n\n  private async deprovisionInstance(instance: CloudInstance): Promise<void> {\n    // Terminate instance\n    \n    if (instance.provider === \'azure\') {\n      await this.azureClient.virtualMachines.delete(\n        \'holovitals-hipaa\',\n        instance.id\n      );\n    } else {\n      await this.awsClient.terminateInstances({\n        InstanceIds: [instance.id]\n      });\n    }\n\n    // Log cost\n    await this.logInstanceCost(instance);\n  }\n\n  private selectProvider(model: string): \'azure\' | \'aws\' {\n    // Select provider based on model availability and cost\n    // For now, default to Azure\n    return \'azure\';\n  }\n\n  private getVMSize(model: string): string {\n    // Return appropriate Azure VM size for model\n    const vmSizes = {\n      \'gpt-4-turbo\': \'Standard_NC6s_v3\',\n      \'gpt-4\': \'Standard_NC6s_v3\',\n      \'claude-3-opus\': \'Standard_NC12s_v3\',\n      \'claude-3-sonnet\': \'Standard_NC6s_v3\',\n      \'llama-3-70b\': \'Standard_NC24s_v3\'\n    };\n\n    return vmSizes[model] || \'Standard_NC6s_v3\';\n  }\n\n  private getInstanceType(model: string): string {\n    // Return appropriate AWS instance type for model\n    const instanceTypes = {\n      \'gpt-4-turbo\': \'p3.2xlarge\',\n      \'gpt-4\': \'p3.2xlarge\',\n      \'claude-3-opus\': \'p3.8xlarge\',\n      \'claude-3-sonnet\': \'p3.2xlarge\',\n      \'llama-3-70b\': \'p3.16xlarge\'\n    };\n\n    return instanceTypes[model] || \'p3.2xlarge\';\n  }\n\n  private async waitForModelReady(instance: CloudInstance): Promise<void> {\n    // Poll instance until model is ready\n    // Typically takes 2-5 minutes\n  }\n\n  private mergeResults(results: any[]): any {\n    // Merge results from split analysis\n    // This depends on the analysis type\n    return {\n      summary: this.generateSummary(results),\n      details: results,\n      merged: true\n    };\n  }\n\n  private generateSummary(results: any[]): string {\n    // Generate summary from multiple results\n    return results.map(r => r.summary).join(\'\\n\\n\');\n  }\n\n  private async logInstanceCost(instance: CloudInstance): Promise<void> {\n    // Calculate and log instance cost\n    const duration = instance.terminatedAt \n      ? (instance.terminatedAt.getTime() - instance.createdAt.getTime()) / 1000 / 60\n      : 0;\n\n    const costPerMinute = this.getCostPerMinute(instance.instanceType);\n    const totalCost = duration * costPerMinute;\n\n    await prisma.instanceCost.create({\n      data: {\n        instanceId: instance.id,\n        provider: instance.provider,\n        instanceType: instance.instanceType,\n        model: instance.model,\n        duration: duration,\n        cost: totalCost,\n        createdAt: instance.createdAt,\n        terminatedAt: instance.terminatedAt\n      }\n    });\n  }\n\n  private getCostPerMinute(instanceType: string): number {\n    // Return cost per minute for instance type\n    const costs = {\n      \'Standard_NC6s_v3\': 0.90 / 60,  // $0.90/hour\n      \'Standard_NC12s_v3\': 1.80 / 60,\n      \'Standard_NC24s_v3\': 3.60 / 60,\n      \'p3.2xlarge\': 3.06 / 60,\n      \'p3.8xlarge\': 12.24 / 60,\n      \'p3.16xlarge\': 24.48 / 60\n    };\n\n    return costs[instanceType] || 1.00 / 60;\n  }\n\n  private async callAzureModel(instance: CloudInstance, params: any): Promise<any> {\n    // Implementation for Azure\n    return {};\n  }\n\n  private async callAWSModel(instance: CloudInstance, params: any): Promise<any> {\n    // Implementation for AWS\n    return {};\n  }\n}\n```\n\n## Cost Analysis\n\n### Lightweight Chatbot\n- **Cost per interaction:** ~$0.002\n- **Expected volume:** 1,000 interactions/day\n- **Monthly cost:** ~$60\n\n### Heavy-Duty Analysis\n- **Cost per analysis:** $0.50-$5.00\n- **Expected volume:** 100 analyses/day\n- **Monthly cost:** ~$5,000-$15,000\n\n### Cloud Infrastructure\n- **Instance cost:** $0.90-$3.06/hour\n- **Average analysis time:** 15 minutes\n- **Cost per analysis:** $0.23-$0.77\n- **Monthly infrastructure cost:** ~$2,300-$7,700\n\n### Total Estimated Monthly Cost\n- **Chatbot:** $60\n- **Analysis:** $5,000-$15,000\n- **Infrastructure:** $2,300-$7,700\n- **Total:** $7,360-$22,760\n\n### Cost Optimization Strategies\n1. Use smaller models when possible (Llama 3, Claude Sonnet)\n2. Cache common analyses\n3. Batch similar analyses\n4. Use spot instances when available\n5. Optimize context windows to reduce tokens\n6. Implement smart escalation to avoid unnecessary deep analysis\n\n## HIPAA Compliance Checklist\n\n✅ **Encrypted Data in Transit** - TLS 1.3\n✅ **Encrypted Data at Rest** - AES-256\n✅ **Access Controls** - Role-based access\n✅ **Audit Logging** - Complete audit trail\n✅ **PHI Sanitization** - Before caching\n✅ **Ephemeral Instances** - No persistent PHI storage\n✅ **Business Associate Agreement** - With cloud providers\n✅ **Breach Notification** - Automated detection\n✅ **Minimum Necessary** - Context optimization\n✅ **Patient Consent** - Required for analysis\n\n## Implementation Timeline\n\n**Week 1-2:** Lightweight chatbot\n**Week 3-4:** Queue system and context optimizer\n**Week 5-6:** Instance provisioner (Azure)\n**Week 7-8:** Instance provisioner (AWS)\n**Week 9-10:** Testing and optimization\n**Week 11-12:** Production deployment\n\n## Next Steps\n\n1. Set up Azure Health Data Services account\n2. Configure HIPAA-compliant infrastructure\n3. Implement lightweight chatbot\n4. Build queue system\n5. Develop context optimizer\n6. Create instance provisioner\n7. Test end-to-end workflow\n8. Deploy to production\n</create-file>\n</function_calls>'}