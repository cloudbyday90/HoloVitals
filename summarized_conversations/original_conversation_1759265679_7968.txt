{'role': 'assistant', 'content': 'Now let\'s create the main LightweightChatbotService:\n\n<function_calls>\n<create-file file_path="medical-analysis-platform/lib/services/LightweightChatbotService.ts">\n/**\n * Lightweight Chatbot Service\n * \n * Fast AI-powered chatbot using GPT-3.5 Turbo for 80% of user queries.\n * Automatically escalates complex queries to heavy-duty analysis.\n * \n * Features:\n * - <2 second response time\n * - Conversation context management\n * - Query complexity classification\n * - Automatic escalation triggers\n * - Cost tracking\n * - Response streaming\n */\n\nimport { PrismaClient } from \'@prisma/client\';\nimport {\n  ChatRequest,\n  ChatResponse,\n  ChatMessage,\n  MessageRole,\n  QueryComplexity,\n  EscalationReason,\n  ChatbotConfig,\n  DEFAULT_CHATBOT_CONFIG,\n  SYSTEM_PROMPTS,\n  TokenUsage\n} from \'@/lib/types/chatbot\';\nimport {\n  createChatCompletion,\n  createStreamingChatCompletion,\n  calculateCost,\n  withRetry\n} from \'@/lib/utils/openai\';\nimport {\n  countMessageTokens,\n  truncateMessages,\n  getAvailableCompletionTokens,\n  getContextWindowSize\n} from \'@/lib/utils/tokenCounter\';\nimport {\n  classifyQueryComplexity,\n  shouldEscalate,\n  getSuggestedFollowUps,\n  estimateClassificationConfidence\n} from \'@/lib/utils/queryClassifier\';\n\nexport class LightweightChatbotService {\n  private prisma: PrismaClient;\n  private config: ChatbotConfig;\n\n  constructor(config: Partial<ChatbotConfig> = {}) {\n    this.prisma = new PrismaClient();\n    this.config = { ...DEFAULT_CHATBOT_CONFIG, ...config };\n  }\n\n  /**\n   * Process a chat request\n   */\n  async chat(request: ChatRequest): Promise<ChatResponse> {\n    const startTime = Date.now();\n\n    try {\n      // Get or create conversation\n      const conversation = await this.getOrCreateConversation(\n        request.conversationId,\n        request.userId\n      );\n\n      // Classify query complexity\n      const complexity = classifyQueryComplexity(request.message);\n      const classificationConfidence = estimateClassificationConfidence(\n        request.message,\n        complexity\n      );\n\n      // Check if escalation is needed\n      const escalationTrigger = shouldEscalate(request.message, {\n        documentCount: request.context?.documentIds?.length,\n        hasTemporalData: request.context?.includeHistory,\n        userRequestedDetailed: false\n      });\n\n      // If escalation needed, return escalation response\n      if (escalationTrigger.shouldEscalate) {\n        return await this.handleEscalation(\n          conversation.id,\n          request.message,\n          escalationTrigger.reason!,\n          complexity,\n          startTime\n        );\n      }\n\n      // Build context messages\n      const contextMessages = await this.buildContextMessages(\n        conversation.id,\n        request.message,\n        request.context\n      );\n\n      // Calculate available tokens\n      const availableTokens = getAvailableCompletionTokens(\n        contextMessages,\n        this.config.model,\n        this.config.maxTokens\n      );\n\n      // Create chat completion\n      const completion = await withRetry(() =>\n        createChatCompletion(contextMessages, {\n          model: this.config.model,\n          maxTokens: availableTokens,\n          temperature: this.config.temperature,\n          topP: this.config.topP,\n          frequencyPenalty: this.config.frequencyPenalty,\n          presencePenalty: this.config.presencePenalty,\n          stream: false\n        })\n      );\n\n      const assistantMessage = completion.choices[0].message.content || \'\';\n      const usage = completion.usage!;\n\n      // Calculate cost\n      const cost = calculateCost(\n        this.config.model,\n        usage.prompt_tokens,\n        usage.completion_tokens\n      );\n\n      // Save messages to database\n      const userMessage = await this.saveMessage(\n        conversation.id,\n        MessageRole.USER,\n        request.message,\n        usage.prompt_tokens,\n        complexity\n      );\n\n      const assistantMessageRecord = await this.saveMessage(\n        conversation.id,\n        MessageRole.ASSISTANT,\n        assistantMessage,\n        usage.completion_tokens\n      );\n\n      // Update conversation\n      await this.updateConversation(conversation.id, cost);\n\n      // Track interaction\n      await this.trackInteraction(\n        conversation.id,\n        request.userId,\n        usage,\n        cost,\n        Date.now() - startTime\n      );\n\n      // Get suggestions\n      const suggestions = getSuggestedFollowUps(request.message, complexity);\n\n      const processingTime = Date.now() - startTime;\n\n      return {\n        conversationId: conversation.id,\n        messageId: assistantMessageRecord.id,\n        content: assistantMessage,\n        role: MessageRole.ASSISTANT,\n        tokens: {\n          prompt: usage.prompt_tokens,\n          completion: usage.completion_tokens,\n          total: usage.total_tokens\n        },\n        cost,\n        complexity,\n        escalated: false,\n        processingTime,\n        model: this.config.model,\n        metadata: {\n          confidence: classificationConfidence,\n          suggestions\n        }\n      };\n    } catch (error: any) {\n      console.error(\'Chat error:\', error);\n      throw new Error(`Chat processing failed: ${error.message}`);\n    }\n  }\n\n  /**\n   * Stream chat response\n   */\n  async *streamChat(request: ChatRequest): AsyncGenerator<string> {\n    try {\n      // Get or create conversation\n      const conversation = await this.getOrCreateConversation(\n        request.conversationId,\n        request.userId\n      );\n\n      // Build context messages\n      const contextMessages = await this.buildContextMessages(\n        conversation.id,\n        request.message,\n        request.context\n      );\n\n      // Stream response\n      let fullResponse = \'\';\n      const stream = createStreamingChatCompletion(contextMessages, {\n        model: this.config.model,\n        maxTokens: this.config.maxTokens,\n        temperature: this.config.temperature,\n        topP: this.config.topP,\n        frequencyPenalty: this.config.frequencyPenalty,\n        presencePenalty: this.config.presencePenalty\n      });\n\n      for await (const chunk of stream) {\n        fullResponse += chunk;\n        yield chunk;\n      }\n\n      // Save messages after streaming completes\n      const complexity = classifyQueryComplexity(request.message);\n      const promptTokens = countMessageTokens(contextMessages, this.config.model);\n      const completionTokens = countMessageTokens(\n        [{ role: \'assistant\', content: fullResponse }],\n        this.config.model\n      );\n\n      await this.saveMessage(\n        conversation.id,\n        MessageRole.USER,\n        request.message,\n        promptTokens,\n        complexity\n      );\n\n      await this.saveMessage(\n        conversation.id,\n        MessageRole.ASSISTANT,\n        fullResponse,\n        completionTokens\n      );\n\n      const cost = calculateCost(\n        this.config.model,\n        promptTokens,\n        completionTokens\n      );\n\n      await this.updateConversation(conversation.id, cost);\n    } catch (error: any) {\n      console.error(\'Streaming error:\', error);\n      throw new Error(`Streaming failed: ${error.message}`);\n    }\n  }\n\n  /**\n   * Get or create conversation\n   */\n  private async getOrCreateConversation(\n    conversationId: string | undefined,\n    userId: string\n  ) {\n    if (conversationId) {\n      const conversation = await this.prisma.chatConversation.findUnique({\n        where: { id: conversationId }\n      });\n\n      if (conversation) {\n        return conversation;\n      }\n    }\n\n    // Create new conversation\n    return await this.prisma.chatConversation.create({\n      data: {\n        userId,\n        title: \'New Conversation\',\n        totalTokens: 0,\n        totalCost: 0,\n        isActive: true\n      }\n    });\n  }\n\n  /**\n   * Build context messages for chat completion\n   */\n  private async buildContextMessages(\n    conversationId: string,\n    currentMessage: string,\n    context?: ChatRequest[\'context\']\n  ): Promise<Array<{ role: string; content: string }>> {\n    const messages: Array<{ role: string; content: string }> = [];\n\n    // Add system prompt\n    let systemPrompt = SYSTEM_PROMPTS.general;\n    if (context?.documentIds && context.documentIds.length > 0) {\n      systemPrompt = SYSTEM_PROMPTS.documentAnalysis;\n    }\n    messages.push({ role: \'system\', content: systemPrompt });\n\n    // Get conversation history\n    if (context?.includeHistory !== false) {\n      const history = await this.prisma.chatMessage.findMany({\n        where: { conversationId },\n        orderBy: { createdAt: \'asc\' },\n        take: this.config.maxContextMessages\n      });\n\n      for (const msg of history) {\n        messages.push({\n          role: msg.role.toLowerCase(),\n          content: msg.content\n        });\n      }\n    }\n\n    // Add current message\n    messages.push({ role: \'user\', content: currentMessage });\n\n    // Truncate if needed\n    const contextWindow = getContextWindowSize(this.config.model);\n    const maxContextTokens = contextWindow - this.config.maxTokens - 100; // Buffer\n\n    return truncateMessages(messages, maxContextTokens, this.config.model, true);\n  }\n\n  /**\n   * Save message to database\n   */\n  private async saveMessage(\n    conversationId: string,\n    role: MessageRole,\n    content: string,\n    tokens: number,\n    complexity?: QueryComplexity\n  ) {\n    return await this.prisma.chatMessage.create({\n      data: {\n        conversationId,\n        role,\n        content,\n        tokens,\n        complexity,\n        escalated: false\n      }\n    });\n  }\n\n  /**\n   * Update conversation totals\n   */\n  private async updateConversation(conversationId: string, additionalCost: number) {\n    const conversation = await this.prisma.chatConversation.findUnique({\n      where: { id: conversationId },\n      include: { messages: true }\n    });\n\n    if (!conversation) return;\n\n    const totalTokens = conversation.messages.reduce(\n      (sum, msg) => sum + msg.tokens,\n      0\n    );\n\n    await this.prisma.chatConversation.update({\n      where: { id: conversationId },\n      data: {\n        totalTokens,\n        totalCost: conversation.totalCost + additionalCost,\n        updatedAt: new Date()\n      }\n    });\n  }\n\n  /**\n   * Track AI interaction\n   */\n  private async trackInteraction(\n    conversationId: string,\n    userId: string,\n    usage: { prompt_tokens: number; completion_tokens: number; total_tokens: number },\n    cost: number,\n    responseTime: number\n  ) {\n    await this.prisma.aIInteraction.create({\n      data: {\n        userId,\n        interactionType: \'CHAT\',\n        model: this.config.model,\n        promptTokens: usage.prompt_tokens,\n        completionTokens: usage.completion_tokens,\n        totalTokens: usage.total_tokens,\n        cost,\n        responseTime,\n        success: true,\n        metadata: { conversationId }\n      }\n    });\n\n    // Track chatbot cost\n    await this.prisma.chatbotCost.create({\n      data: {\n        conversationId,\n        model: this.config.model,\n        promptTokens: usage.prompt_tokens,\n        completionTokens: usage.completion_tokens,\n        totalTokens: usage.total_tokens,\n        cost\n      }\n    });\n  }\n\n  /**\n   * Handle escalation to heavy-duty analysis\n   */\n  private async handleEscalation(\n    conversationId: string,\n    message: string,\n    reason: EscalationReason,\n    complexity: QueryComplexity,\n    startTime: number\n  ): Promise<ChatResponse> {\n    // Save user message with escalation flag\n    const userMessage = await this.prisma.chatMessage.create({\n      data: {\n        conversationId,\n        role: MessageRole.USER,\n        content: message,\n        tokens: countMessageTokens([{ role: \'user\', content: message }], this.config.model),\n        complexity,\n        escalated: true,\n        escalationReason: reason\n      }\n    });\n\n    // Create escalation response\n    const escalationMessage = this.getEscalationMessage(reason);\n    \n    const assistantMessage = await this.prisma.chatMessage.create({\n      data: {\n        conversationId,\n        role: MessageRole.ASSISTANT,\n        content: escalationMessage,\n        tokens: countMessageTokens(\n          [{ role: \'assistant\', content: escalationMessage }],\n          this.config.model\n        ),\n        escalated: true,\n        escalationReason: reason\n      }\n    });\n\n    const processingTime = Date.now() - startTime;\n\n    return {\n      conversationId,\n      messageId: assistantMessage.id,\n      content: escalationMessage,\n      role: MessageRole.ASSISTANT,\n      tokens: {\n        prompt: userMessage.tokens,\n        completion: assistantMessage.tokens,\n        total: userMessage.tokens + assistantMessage.tokens\n      },\n      cost: 0, // No API cost for escalation message\n      complexity,\n      escalated: true,\n      escalationReason: reason,\n      processingTime,\n      model: this.config.model,\n      metadata: {\n        requiresDetailedAnalysis: true\n      }\n    };\n  }\n\n  /**\n   * Get escalation message based on reason\n   */\n  private getEscalationMessage(reason: EscalationReason): string {\n    const messages: Record<EscalationReason, string> = {\n      [EscalationReason.COMPLEXITY]: \n        \'This query requires detailed analysis. I\\\'m creating a comprehensive analysis request for you. This will take a few minutes but will provide much more thorough insights.\',\n      [EscalationReason.MEDICAL_ANALYSIS]:\n        \'This requires detailed medical analysis. I\\\'m preparing a comprehensive medical review that will analyze all relevant information and provide detailed insights.\',\n      [EscalationReason.MULTIPLE_DOCUMENTS]:\n        \'You\\\'re asking about multiple documents. I\\\'m creating a cross-document analysis that will compare and synthesize information from all relevant sources.\',\n      [EscalationReason.TREND_ANALYSIS]:\n        \'This requires temporal trend analysis. I\\\'m preparing a detailed analysis of changes over time with visualizations and insights.\',\n      [EscalationReason.UNCERTAINTY]:\n        \'I want to provide you with the most accurate information. I\\\'m escalating this to our detailed analysis system for a more thorough review.\',\n      [EscalationReason.USER_REQUEST]:\n        \'I\\\'m creating a detailed analysis as requested. This will provide comprehensive insights and recommendations.\'\n    };\n\n    return messages[reason] || messages[EscalationReason.COMPLEXITY];\n  }\n\n  /**\n   * Get conversation history\n   */\n  async getConversationHistory(conversationId: string) {\n    return await this.prisma.chatConversation.findUnique({\n      where: { id: conversationId },\n      include: {\n        messages: {\n          orderBy: { createdAt: \'asc\' }\n        }\n      }\n    });\n  }\n\n  /**\n   * Get user conversations\n   */\n  async getUserConversations(userId: string) {\n    return await this.prisma.chatConversation.findMany({\n      where: { userId },\n      orderBy: { updatedAt: \'desc\' },\n      include: {\n        messages: {\n          take: 1,\n          orderBy: { createdAt: \'desc\' }\n        }\n      }\n    });\n  }\n\n  /**\n   * Delete conversation\n   */\n  async deleteConversation(conversationId: string) {\n    await this.prisma.chatMessage.deleteMany({\n      where: { conversationId }\n    });\n\n    await this.prisma.chatConversation.delete({\n      where: { id: conversationId }\n    });\n  }\n\n  /**\n   * Cleanup resources\n   */\n  async cleanup() {\n    await this.prisma.$disconnect();\n  }\n}\n</create-file>\n</function_calls>'}