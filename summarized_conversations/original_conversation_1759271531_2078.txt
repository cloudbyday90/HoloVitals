{'role': 'assistant', 'content': '<function_calls>\n<str-replace file_path="medical-analysis-platform/lib/types/ai-provider.ts">\n    <old_str>  [AIModel.CLAUDE_35_SONNET]: {\n    supportsStreaming: true,\n    supportsVision: true,\n    supportsFunctionCalling: true,\n    supportsSystemMessages: true,\n    maxContextWindow: 200000,\n    maxOutputTokens: 8192\n  }\n};</old_str>\n    <new_str>  [AIModel.CLAUDE_35_SONNET]: {\n    supportsStreaming: true,\n    supportsVision: true,\n    supportsFunctionCalling: true,\n    supportsSystemMessages: true,\n    maxContextWindow: 200000,\n    maxOutputTokens: 8192\n  },\n  [AIModel.CLAUDE_35_SONNET_V2]: {\n    supportsStreaming: true,\n    supportsVision: true,\n    supportsFunctionCalling: true,\n    supportsSystemMessages: true,\n    maxContextWindow: 200000,\n    maxOutputTokens: 8192\n  },\n  \n  // Llama Models (via Open WebUI/Ollama)\n  [AIModel.LLAMA_32_1B]: {\n    supportsStreaming: true,\n    supportsVision: false,\n    supportsFunctionCalling: false,\n    supportsSystemMessages: true,\n    maxContextWindow: 128000,\n    maxOutputTokens: 4096\n  },\n  [AIModel.LLAMA_32_3B]: {\n    supportsStreaming: true,\n    supportsVision: false,\n    supportsFunctionCalling: false,\n    supportsSystemMessages: true,\n    maxContextWindow: 128000,\n    maxOutputTokens: 4096\n  },\n  [AIModel.LLAMA_32_11B]: {\n    supportsStreaming: true,\n    supportsVision: true,\n    supportsFunctionCalling: false,\n    supportsSystemMessages: true,\n    maxContextWindow: 128000,\n    maxOutputTokens: 4096\n  },\n  [AIModel.LLAMA_32_90B]: {\n    supportsStreaming: true,\n    supportsVision: true,\n    supportsFunctionCalling: false,\n    supportsSystemMessages: true,\n    maxContextWindow: 128000,\n    maxOutputTokens: 4096\n  }\n};</new_str>\n</str-replace>\n</function_calls>'}