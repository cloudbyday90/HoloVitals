{'role': 'assistant', 'content': 'Now let\'s create the refactored OpenAI Provider:\n\n<function_calls>\n<create-file file_path="medical-analysis-platform/lib/providers/OpenAIProvider.ts">\n/**\n * OpenAI AI Provider\n * \n * Implementation of IAIProvider for OpenAI\'s GPT models.\n * Supports GPT-3.5 Turbo, GPT-4, GPT-4 Turbo, and GPT-4o.\n */\n\nimport OpenAI from \'openai\';\nimport { encoding_for_model, TiktokenModel } from \'tiktoken\';\nimport {\n  IAIProvider,\n  AIProvider,\n  AIModel,\n  AICompletionRequest,\n  AICompletionResponse,\n  AIStreamChunk,\n  AIProviderConfig,\n  AIProviderCapabilities,\n  AIProviderCosts,\n  MODEL_CAPABILITIES,\n  MODEL_COSTS,\n  AIProviderError,\n  RateLimitError,\n  AuthenticationError,\n  InvalidRequestError\n} from \'@/lib/types/ai-provider\';\n\nexport class OpenAIProvider implements IAIProvider {\n  private client: OpenAI;\n  private config: AIProviderConfig;\n  private encoding: any;\n\n  readonly provider = AIProvider.OPENAI;\n  readonly model: AIModel;\n  readonly capabilities: AIProviderCapabilities;\n  readonly costs: AIProviderCosts;\n\n  constructor(config: AIProviderConfig) {\n    if (config.provider !== AIProvider.OPENAI) {\n      throw new Error(\'Invalid provider for OpenAIProvider\');\n    }\n\n    this.config = config;\n    this.model = config.model;\n    this.capabilities = MODEL_CAPABILITIES[config.model];\n    this.costs = MODEL_COSTS[config.model];\n\n    // Initialize OpenAI client\n    this.client = new OpenAI({\n      apiKey: config.apiKey,\n      baseURL: config.baseURL,\n      timeout: config.timeout || 60000,\n    });\n\n    // Initialize tokenizer\n    try {\n      this.encoding = encoding_for_model(this.model as TiktokenModel);\n    } catch (error) {\n      // Fallback to gpt-3.5-turbo encoding\n      this.encoding = encoding_for_model(\'gpt-3.5-turbo\');\n    }\n  }\n\n  /**\n   * Complete a chat request\n   */\n  async complete(request: AICompletionRequest): Promise<AICompletionResponse> {\n    try {\n      const response = await this.client.chat.completions.create({\n        model: request.model,\n        messages: request.messages as any,\n        max_tokens: request.maxTokens || this.config.maxTokens || 4096,\n        temperature: request.temperature ?? this.config.temperature ?? 0.7,\n        top_p: request.topP ?? this.config.topP ?? 1,\n        stop: request.stopSequences,\n        stream: false\n      });\n\n      const choice = response.choices[0];\n      const content = choice.message.content || \'\';\n      const usage = response.usage!;\n\n      // Calculate cost\n      const cost = this.calculateCost(\n        usage.prompt_tokens,\n        usage.completion_tokens\n      );\n\n      return {\n        id: response.id,\n        content,\n        model: request.model,\n        provider: this.provider,\n        usage: {\n          promptTokens: usage.prompt_tokens,\n          completionTokens: usage.completion_tokens,\n          totalTokens: usage.total_tokens\n        },\n        cost,\n        finishReason: this.mapFinishReason(choice.finish_reason),\n        metadata: {\n          model: response.model,\n          systemFingerprint: response.system_fingerprint\n        }\n      };\n    } catch (error: any) {\n      throw this.handleError(error);\n    }\n  }\n\n  /**\n   * Stream a chat request\n   */\n  async *stream(request: AICompletionRequest): AsyncGenerator<AIStreamChunk> {\n    try {\n      const stream = await this.client.chat.completions.create({\n        model: request.model,\n        messages: request.messages as any,\n        max_tokens: request.maxTokens || this.config.maxTokens || 4096,\n        temperature: request.temperature ?? this.config.temperature ?? 0.7,\n        top_p: request.topP ?? this.config.topP ?? 1,\n        stop: request.stopSequences,\n        stream: true\n      });\n\n      for await (const chunk of stream) {\n        const delta = chunk.choices[0]?.delta;\n        const finishReason = chunk.choices[0]?.finish_reason;\n\n        if (delta?.content) {\n          yield {\n            content: delta.content\n          };\n        }\n\n        if (finishReason) {\n          yield {\n            content: \'\',\n            finishReason: this.mapFinishReason(finishReason)\n          };\n        }\n      }\n    } catch (error: any) {\n      throw this.handleError(error);\n    }\n  }\n\n  /**\n   * Count tokens in text using tiktoken\n   */\n  countTokens(text: string): number {\n    try {\n      const tokens = this.encoding.encode(text);\n      return tokens.length;\n    } catch (error) {\n      // Fallback estimation\n      return Math.ceil(text.length / 4);\n    }\n  }\n\n  /**\n   * Calculate cost based on token usage\n   */\n  calculateCost(promptTokens: number, completionTokens: number): number {\n    const promptCost = (promptTokens / 1_000_000) * this.costs.promptCostPer1M;\n    const completionCost = (completionTokens / 1_000_000) * this.costs.completionCostPer1M;\n    return promptCost + completionCost;\n  }\n\n  /**\n   * Validate API key\n   */\n  async validateApiKey(): Promise<boolean> {\n    try {\n      await this.client.models.list();\n      return true;\n    } catch (error: any) {\n      if (error.status === 401) {\n        return false;\n      }\n      return true;\n    }\n  }\n\n  /**\n   * Update configuration\n   */\n  updateConfig(config: Partial<AIProviderConfig>): void {\n    this.config = { ...this.config, ...config };\n    \n    // Reinitialize client if API key or baseURL changed\n    if (config.apiKey || config.baseURL || config.timeout) {\n      this.client = new OpenAI({\n        apiKey: this.config.apiKey,\n        baseURL: this.config.baseURL,\n        timeout: this.config.timeout || 60000,\n      });\n    }\n\n    // Reinitialize encoding if model changed\n    if (config.model) {\n      try {\n        this.encoding.free();\n        this.encoding = encoding_for_model(config.model as TiktokenModel);\n      } catch (error) {\n        this.encoding = encoding_for_model(\'gpt-3.5-turbo\');\n      }\n    }\n  }\n\n  /**\n   * Get current configuration\n   */\n  getConfig(): AIProviderConfig {\n    return { ...this.config };\n  }\n\n  /**\n   * Map OpenAI\'s finish reason to our standard format\n   */\n  private mapFinishReason(\n    finishReason: string | null\n  ): \'stop\' | \'length\' | \'content_filter\' | \'tool_use\' {\n    switch (finishReason) {\n      case \'stop\':\n        return \'stop\';\n      case \'length\':\n        return \'length\';\n      case \'content_filter\':\n        return \'content_filter\';\n      case \'tool_calls\':\n      case \'function_call\':\n        return \'tool_use\';\n      default:\n        return \'stop\';\n    }\n  }\n\n  /**\n   * Handle errors from OpenAI API\n   */\n  private handleError(error: any): AIProviderError {\n    if (error instanceof OpenAI.APIError) {\n      const status = error.status;\n      const message = error.message;\n\n      if (status === 401) {\n        return new AuthenticationError(this.provider);\n      } else if (status === 429) {\n        return new RateLimitError(this.provider);\n      } else if (status === 400) {\n        return new InvalidRequestError(this.provider, message);\n      } else {\n        return new AIProviderError(\n          `OpenAI API error: ${message}`,\n          this.provider,\n          status,\n          error\n        );\n      }\n    }\n\n    return new AIProviderError(\n      `Unexpected error: ${error.message}`,\n      this.provider,\n      undefined,\n      error\n    );\n  }\n\n  /**\n   * Cleanup resources\n   */\n  cleanup(): void {\n    if (this.encoding) {\n      this.encoding.free();\n    }\n  }\n}\n</create-file>\n</function_calls>'}