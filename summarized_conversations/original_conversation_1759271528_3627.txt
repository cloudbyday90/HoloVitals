{'role': 'user', 'content': '<tool_result> <execute-command> ToolResult(success=True, output=\'{\\n  "output": "   export const MODEL_CAPABILITIES: Record<AIModel, AIProviderCapabilities> = {\\\\n     // OpenAI Models\\\\n     [AIModel.GPT_35_TURBO]: {\\\\n       supportsStreaming: true,\\\\n       supportsVision: false,\\\\n       supportsFunctionCalling: true,\\\\n       supportsSystemMessages: true,\\\\n       maxContextWindow: 16385,\\\\n       maxOutputTokens: 4096\\\\n     },\\\\n     [AIModel.GPT_4]: {\\\\n       supportsStreaming: true,\\\\n       supportsVision: false,\\\\n       supportsFunctionCalling: true,\\\\n       supportsSystemMessages: true,\\\\n       maxContextWindow: 8192,\\\\n       maxOutputTokens: 4096\\\\n     },\\\\n     [AIModel.GPT_4_TURBO]: {\\\\n       supportsStreaming: true,\\\\n       supportsVision: true,\\\\n       supportsFunctionCalling: true,\\\\n       supportsSystemMessages: true,\\\\n       maxContextWindow: 128000,\\\\n       maxOutputTokens: 4096\\\\n     },\\\\n     [AIModel.GPT_4O]: {\\\\n       supportsStreaming: true,\\\\n       supportsVision: true,\\\\n       supportsFunctionCalling: true,\\\\n       supportsSystemMessages: true,\\\\n       maxContextWindow: 128000,\\\\n       maxOutputTokens: 4096\\\\n     },\\\\n     \\\\n     // Claude Models\\\\n     [AIModel.CLAUDE_3_OPUS]: {\\\\n       supportsStreaming: true,\\\\n       supportsVision: true,\\\\n       supportsFunctionCalling: true,\\\\n       supportsSystemMessages: true,\\\\n       maxContextWindow: 200000,\\\\n       maxOutputTokens: 4096\\\\n     },\\\\n     [AIModel.CLAUDE_3_SONNET]: {\\\\n       supportsStreaming: true,\\\\n       supportsVision: true,\\\\n       supportsFunctionCalling: true,\\\\n       supportsSystemMessages: true,\\\\n       maxContextWindow: 200000,\\\\n       maxOutputTokens: 4096\\\\n     },\\\\n     [AIModel.CLAUDE_3_HAIKU]: {\\\\n       supportsStreaming: true,\\\\n       supportsVision: true,\\\\n       supportsFunctionCalling: true,\\\\n       supportsSystemMessages: true,\\\\n       maxContextWindow: 200000,\\\\n       maxOutputTokens: 4096\\\\n     },\\\\n     [AIModel.CLAUDE_35_SONNET]: {\\\\n       supportsStreaming: true,\\\\n       supportsVision: true,\\\\n       supportsFunctionCalling: true,\\\\n       supportsSystemMessages: true,\\\\n       maxContextWindow: 200000,\\\\n       maxOutputTokens: 8192\\\\n     }\\\\n   };\\\\n   \\\\n   /**\\\\n    * Model costs (per 1M tokens)\\\\n    */\\\\n   export const MODEL_COSTS: Record<AIModel, AIProviderCosts> = {\\\\n     // OpenAI Models\\\\n     [AIModel.GPT_35_TURBO]: {\\\\n       promptCostPer1M: 0.50,\\\\n       completionCostPer1M: 1.50,\\\\n       currency: \\\'USD\\\'\\\\n     },\\\\n     [AIModel.GPT_4]: {\\\\n       promptCostPer1M: 30.00,\\\\n       completionCostPer1M: 60.00,\\\\n       currency: \\\'USD\\\'\\\\n     },\\\\n     [AIModel.GPT_4_TURBO]: {\\\\n       promptCostPer1M: 10.00,\\\\n       completionCostPer1M: 30.00,\\\\n       currency: \\\'USD\\\'\\\\n     },\\\\n     [AIModel.GPT_4O]: {\\\\n       promptCostPer1M: 5.00,\\\\n       completionCostPer1M: 15.00,\\\\n       currency: \\\'USD\\\'\\\\n     },\\\\n     \\\\n     // Claude Models\\\\n     [AIModel.CLAUDE_3_OPUS]: {\\\\n       promptCostPer1M: 15.00,\\\\n       completionCostPer1M: 75.00,\\\\n",\\n  "exit_code": 0,\\n  "cwd": "/workspace"\\n}\') </execute-command> </tool_result>'}