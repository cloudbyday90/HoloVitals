{'role': 'assistant', 'content': 'Now let\'s create documentation for the LightweightChatbotService:\n\n<function_calls>\n<create-file file_path="medical-analysis-platform/docs/LIGHTWEIGHT_CHATBOT_SERVICE.md">\n# LightweightChatbotService Documentation\n\n## Overview\n\nThe LightweightChatbotService provides fast AI-powered chat responses using GPT-3.5 Turbo for 80% of user queries. It automatically escalates complex queries to heavy-duty analysis when needed.\n\n## Features\n\n- ✅ **Fast Response Time**: <2 seconds average\n- ✅ **Cost Efficient**: Uses GPT-3.5 Turbo ($0.50-$1.50 per 1M tokens)\n- ✅ **Smart Escalation**: Automatically detects complex queries\n- ✅ **Context Management**: Maintains conversation history\n- ✅ **Token Optimization**: Intelligent context window management\n- ✅ **Streaming Support**: Real-time response streaming\n- ✅ **Cost Tracking**: Tracks all API costs\n- ✅ **Error Handling**: Robust retry logic and error recovery\n\n## Architecture\n\n```\nUser Query\n    ↓\nQuery Classification\n    ↓\nEscalation Check\n    ↓\n┌─────────────┬─────────────┐\n│   Simple    │   Complex   │\n│  (80%)      │   (20%)     │\n├─────────────┼─────────────┤\n│ GPT-3.5     │  Escalate   │\n│ Turbo       │  to Heavy   │\n│ <2 sec      │  Analysis   │\n└─────────────┴─────────────┘\n    ↓\nResponse + Cost Tracking\n```\n\n## Usage\n\n### Basic Chat\n\n```typescript\nimport { LightweightChatbotService } from \'@/lib/services/LightweightChatbotService\';\n\nconst chatbot = new LightweightChatbotService();\n\nconst response = await chatbot.chat({\n  userId: \'user-123\',\n  message: \'What is my blood pressure?\'\n});\n\nconsole.log(response.content);\n// "Your most recent blood pressure reading was 120/80 mmHg, which is in the normal range..."\n```\n\n### With Conversation Context\n\n```typescript\nconst response = await chatbot.chat({\n  conversationId: \'conv-123\', // Continue existing conversation\n  userId: \'user-123\',\n  message: \'What about my cholesterol?\',\n  context: {\n    includeHistory: true, // Include previous messages\n    documentIds: [\'doc-1\', \'doc-2\'], // Reference specific documents\n    patientId: \'patient-123\'\n  }\n});\n```\n\n### Streaming Response\n\n```typescript\nfor await (const chunk of chatbot.streamChat({\n  userId: \'user-123\',\n  message: \'Explain my lab results\'\n})) {\n  process.stdout.write(chunk);\n}\n```\n\n### Get Conversation History\n\n```typescript\nconst conversation = await chatbot.getConversationHistory(\'conv-123\');\n\nconsole.log(conversation.messages);\n// [\n//   { role: \'USER\', content: \'What is my blood pressure?\' },\n//   { role: \'ASSISTANT\', content: \'Your blood pressure is...\' }\n// ]\n```\n\n### Get User Conversations\n\n```typescript\nconst conversations = await chatbot.getUserConversations(\'user-123\');\n\nconversations.forEach(conv => {\n  console.log(`${conv.title}: ${conv.messages[0].content}`);\n});\n```\n\n### Delete Conversation\n\n```typescript\nawait chatbot.deleteConversation(\'conv-123\');\n```\n\n## API Endpoint\n\n### POST /api/chat\n\nSend a message to the chatbot.\n\n**Request:**\n```json\n{\n  "userId": "user-123",\n  "message": "What is my blood pressure?",\n  "conversationId": "conv-123", // Optional\n  "context": {\n    "documentIds": ["doc-1"],\n    "patientId": "patient-123",\n    "includeHistory": true\n  }\n}\n```\n\n**Response:**\n```json\n{\n  "conversationId": "conv-123",\n  "messageId": "msg-456",\n  "content": "Your blood pressure is 120/80 mmHg...",\n  "role": "ASSISTANT",\n  "tokens": {\n    "prompt": 50,\n    "completion": 20,\n    "total": 70\n  },\n  "cost": 0.00105,\n  "complexity": "SIMPLE",\n  "escalated": false,\n  "processingTime": 1234,\n  "model": "gpt-3.5-turbo",\n  "metadata": {\n    "confidence": 0.9,\n    "suggestions": [\n      "Would you like more information?"\n    ]\n  }\n}\n```\n\n### GET /api/chat?conversationId=xxx\n\nGet conversation history.\n\n**Response:**\n```json\n{\n  "id": "conv-123",\n  "userId": "user-123",\n  "title": "Blood Pressure Discussion",\n  "messages": [\n    {\n      "id": "msg-1",\n      "role": "USER",\n      "content": "What is my blood pressure?",\n      "tokens": 10,\n      "createdAt": "2025-09-30T12:00:00Z"\n    },\n    {\n      "id": "msg-2",\n      "role": "ASSISTANT",\n      "content": "Your blood pressure is...",\n      "tokens": 20,\n      "createdAt": "2025-09-30T12:00:02Z"\n    }\n  ],\n  "totalTokens": 30,\n  "totalCost": 0.00045,\n  "isActive": true\n}\n```\n\n### GET /api/chat?userId=xxx\n\nGet all conversations for a user.\n\n**Response:**\n```json\n[\n  {\n    "id": "conv-1",\n    "title": "Blood Pressure Discussion",\n    "messages": [\n      { "content": "Latest message..." }\n    ],\n    "updatedAt": "2025-09-30T12:00:00Z"\n  }\n]\n```\n\n### DELETE /api/chat?conversationId=xxx\n\nDelete a conversation.\n\n**Response:**\n```json\n{\n  "message": "Conversation deleted successfully"\n}\n```\n\n## Query Classification\n\nThe service automatically classifies queries into four complexity levels:\n\n### SIMPLE\n- Single fact questions\n- Definition requests\n- Normal range queries\n- **Examples:**\n  - "What is my blood pressure?"\n  - "Define hypertension"\n  - "What is the normal range for glucose?"\n\n### MODERATE\n- Comparison requests\n- Historical queries\n- Related information\n- **Examples:**\n  - "Compare my current and previous results"\n  - "What\'s the difference between HDL and LDL?"\n  - "Show my blood pressure history"\n\n### COMPLEX\n- Multi-document analysis\n- Correlation requests\n- Predictive queries\n- **Examples:**\n  - "Analyze all my lab results"\n  - "What\'s the correlation between my diet and cholesterol?"\n  - "Predict my risk of diabetes"\n\n### CRITICAL\n- Emergency indicators\n- Urgent medical concerns\n- Life-threatening conditions\n- **Examples:**\n  - "I\'m having chest pain"\n  - "Is this result critical?"\n  - "Emergency symptoms"\n\n## Escalation Triggers\n\nQueries are escalated to heavy-duty analysis when:\n\n1. **Complexity Score > 0.7**: Query is too complex for quick response\n2. **Multiple Documents (≥3)**: Cross-document analysis needed\n3. **Temporal Analysis**: Trend analysis over time required\n4. **Medical Analysis**: Detailed medical interpretation needed\n5. **Uncertainty**: AI confidence < 0.5\n6. **User Request**: User explicitly requests detailed analysis\n\n### Escalation Response\n\nWhen escalated, the chatbot returns:\n\n```json\n{\n  "escalated": true,\n  "escalationReason": "COMPLEXITY",\n  "content": "This query requires detailed analysis. I\'m creating a comprehensive analysis request...",\n  "metadata": {\n    "requiresDetailedAnalysis": true\n  }\n}\n```\n\n## Configuration\n\n### Default Configuration\n\n```typescript\n{\n  model: \'gpt-3.5-turbo\',\n  maxTokens: 500,\n  temperature: 0.7,\n  topP: 1,\n  frequencyPenalty: 0,\n  presencePenalty: 0,\n  maxContextMessages: 10,\n  escalationThreshold: 0.7,\n  streamResponse: true\n}\n```\n\n### Custom Configuration\n\n```typescript\nconst chatbot = new LightweightChatbotService({\n  model: \'gpt-3.5-turbo-16k\', // Use 16K context window\n  maxTokens: 1000,             // Longer responses\n  temperature: 0.5,            // More focused responses\n  maxContextMessages: 20       // More conversation history\n});\n```\n\n## Cost Tracking\n\nAll API calls are tracked in the database:\n\n### Token Costs (per 1M tokens)\n- **GPT-3.5 Turbo**: $0.50 (prompt) + $1.50 (completion)\n- **GPT-4**: $30 (prompt) + $60 (completion)\n- **GPT-4 Turbo**: $10 (prompt) + $30 (completion)\n\n### Cost Calculation\n\n```typescript\nconst cost = \n  (promptTokens / 1000) * promptCost +\n  (completionTokens / 1000) * completionCost;\n```\n\n### Example Costs\n\n| Query Type | Tokens | Cost |\n|------------|--------|------|\n| Simple question | 70 | $0.00105 |\n| Moderate query | 150 | $0.00225 |\n| Complex analysis | 500 | $0.00750 |\n\n## Performance Metrics\n\n### Target Metrics\n- **Response Time**: <2 seconds (95th percentile)\n- **Escalation Rate**: ~20%\n- **Success Rate**: >99%\n- **Cost per Query**: <$0.01\n\n### Monitoring\n\n```typescript\n// Track metrics in database\nawait prisma.aIInteraction.create({\n  data: {\n    userId,\n    interactionType: \'CHAT\',\n    model: \'gpt-3.5-turbo\',\n    promptTokens,\n    completionTokens,\n    totalTokens,\n    cost,\n    responseTime,\n    success: true\n  }\n});\n```\n\n## Error Handling\n\n### Automatic Retries\n\nThe service automatically retries failed requests up to 3 times with exponential backoff:\n\n```typescript\nawait withRetry(\n  () => createChatCompletion(messages),\n  maxRetries: 3,\n  delayMs: 1000\n);\n```\n\n### Error Types\n\n1. **Rate Limit (429)**: Automatic retry with backoff\n2. **Authentication (401)**: Immediate failure, check API key\n3. **Server Error (500)**: Retry up to 3 times\n4. **Timeout**: Retry with increased timeout\n\n### Error Response\n\n```json\n{\n  "error": "Rate limit exceeded. Please try again later."\n}\n```\n\n## Best Practices\n\n### 1. Use Conversation Context\n\n```typescript\n// ✅ Good: Maintain conversation context\nconst response = await chatbot.chat({\n  conversationId: existingConversationId,\n  userId,\n  message: \'Follow-up question\'\n});\n\n// ❌ Bad: Start new conversation for every message\nconst response = await chatbot.chat({\n  userId,\n  message: \'Follow-up question\'\n});\n```\n\n### 2. Include Relevant Documents\n\n```typescript\n// ✅ Good: Reference specific documents\nconst response = await chatbot.chat({\n  userId,\n  message: \'Explain my lab results\',\n  context: {\n    documentIds: [\'lab-results-2025-09-30\']\n  }\n});\n```\n\n### 3. Handle Escalations\n\n```typescript\nconst response = await chatbot.chat(request);\n\nif (response.escalated) {\n  // Queue for heavy-duty analysis\n  await analysisQueue.add({\n    userId: request.userId,\n    query: request.message,\n    priority: \'HIGH\'\n  });\n}\n```\n\n### 4. Stream Long Responses\n\n```typescript\n// ✅ Good: Stream for better UX\nfor await (const chunk of chatbot.streamChat(request)) {\n  updateUI(chunk);\n}\n\n// ❌ Bad: Wait for complete response\nconst response = await chatbot.chat(request);\nupdateUI(response.content);\n```\n\n### 5. Clean Up Resources\n\n```typescript\n// Always cleanup when done\nawait chatbot.cleanup();\n```\n\n## Testing\n\n### Run Tests\n\n```bash\nnpm test\n```\n\n### Run with Coverage\n\n```bash\nnpm run test:coverage\n```\n\n### Watch Mode\n\n```bash\nnpm run test:watch\n```\n\n## Database Schema\n\n### Tables Used\n\n1. **chat_conversations**: Conversation metadata\n2. **chat_messages**: Individual messages\n3. **ai_interactions**: API call tracking\n4. **chatbot_costs**: Cost tracking\n\n### Example Queries\n\n```sql\n-- Get conversation with messages\nSELECT c.*, m.*\nFROM chat_conversations c\nLEFT JOIN chat_messages m ON m.conversation_id = c.id\nWHERE c.id = \'conv-123\'\nORDER BY m.created_at ASC;\n\n-- Get total cost by user\nSELECT user_id, SUM(total_cost) as total_cost\nFROM chat_conversations\nGROUP BY user_id;\n\n-- Get escalation rate\nSELECT \n  COUNT(CASE WHEN escalated = true THEN 1 END)::float / COUNT(*) as escalation_rate\nFROM chat_messages\nWHERE role = \'USER\';\n```\n\n## Troubleshooting\n\n### Issue: Slow Response Times\n\n**Solution:**\n1. Check OpenAI API status\n2. Reduce `maxContextMessages`\n3. Use streaming for better perceived performance\n\n### Issue: High Costs\n\n**Solution:**\n1. Review `maxTokens` setting\n2. Implement more aggressive escalation\n3. Optimize system prompts\n4. Use context truncation\n\n### Issue: Frequent Escalations\n\n**Solution:**\n1. Adjust `escalationThreshold`\n2. Review query classification logic\n3. Improve system prompts\n4. Add more training examples\n\n## Future Enhancements\n\n- [ ] Multi-language support\n- [ ] Voice input/output\n- [ ] Image analysis integration\n- [ ] Custom model fine-tuning\n- [ ] Advanced caching strategies\n- [ ] Conversation summarization\n- [ ] Sentiment analysis\n- [ ] Intent detection\n\n## Support\n\nFor issues or questions:\n- GitHub Issues: https://github.com/cloudbyday90/HoloVitals/issues\n- Documentation: /docs\n- Email: support@holovitals.com\n</create-file>\n</function_calls>'}