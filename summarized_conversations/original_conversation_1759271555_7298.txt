{'role': 'assistant', 'content': 'Now let me create a quick setup guide for local Llama deployment:\n\n<function_calls>\n<create-file file_path="medical-analysis-platform/docs/LLAMA_SETUP_GUIDE.md">\n# Llama Local LLM Setup Guide\n\n## Why Use Local Llama Models?\n\n### Benefits\n- âœ… **FREE** - No API costs, unlimited usage\n- âœ… **PRIVATE** - Data never leaves your infrastructure\n- âœ… **OFFLINE** - Works without internet connection\n- âœ… **HIPAA-COMPLIANT** - Complete data control\n- âœ… **FAST** - No network latency\n- âœ… **CUSTOMIZABLE** - Fine-tune for your needs\n\n### Use Cases\n- Medical data analysis (HIPAA compliance)\n- Development and testing\n- High-volume processing\n- Sensitive patient information\n- Offline deployments\n- Cost optimization\n\n## Quick Start (5 Minutes)\n\n### Option 1: Docker Compose (Recommended)\n\n1. **Create docker-compose.yml**:\n```yaml\nversion: \'3.8\'\n\nservices:\n  ollama:\n    image: ollama/ollama:latest\n    container_name: ollama\n    ports:\n      - "11434:11434"\n    volumes:\n      - ollama_data:/root/.ollama\n    restart: unless-stopped\n\n  open-webui:\n    image: ghcr.io/open-webui/open-webui:main\n    container_name: open-webui\n    ports:\n      - "3000:8080"\n    environment:\n      - OLLAMA_BASE_URL=http://ollama:11434\n    volumes:\n      - open-webui_data:/app/backend/data\n    depends_on:\n      - ollama\n    restart: unless-stopped\n\nvolumes:\n  ollama_data:\n  open-webui_data:\n```\n\n2. **Start services**:\n```bash\ndocker-compose up -d\n```\n\n3. **Pull Llama models**:\n```bash\ndocker exec -it ollama ollama pull llama3.2:90b\ndocker exec -it ollama ollama pull llama3.2:11b\ndocker exec -it ollama ollama pull llama3.2:3b\ndocker exec -it ollama ollama pull llama3.2:1b\n```\n\n4. **Configure HoloVitals** - Add to `.env`:\n```bash\nOPEN_WEBUI_URL="http://localhost:3000/api"\n```\n\n5. **Test**:\n```bash\ncurl http://localhost:3000/api/health\n```\n\n### Option 2: Native Installation\n\n#### Step 1: Install Ollama\n\n**Linux/Mac**:\n```bash\ncurl -fsSL https://ollama.com/install.sh | sh\n```\n\n**Windows**:\nDownload from: https://ollama.com/download/windows\n\n#### Step 2: Start Ollama\n```bash\nollama serve\n```\n\n#### Step 3: Pull Models\n```bash\n# Large model (best quality, requires 64GB RAM)\nollama pull llama3.2:90b\n\n# Medium model (good balance, requires 16GB RAM)\nollama pull llama3.2:11b\n\n# Small model (fast, requires 4GB RAM)\nollama pull llama3.2:3b\n\n# Tiny model (ultra-fast, requires 2GB RAM)\nollama pull llama3.2:1b\n```\n\n#### Step 4: Install Open WebUI (Optional)\n```bash\ndocker run -d -p 3000:8080 \\\n  -e OLLAMA_BASE_URL=http://host.docker.internal:11434 \\\n  -v open-webui:/app/backend/data \\\n  --name open-webui \\\n  ghcr.io/open-webui/open-webui:main\n```\n\n#### Step 5: Configure HoloVitals\nAdd to `.env`:\n```bash\nLLAMA_BASE_URL="http://localhost:11434/api"\n# OR (if using Open WebUI)\nOPEN_WEBUI_URL="http://localhost:3000/api"\n```\n\n## System Requirements\n\n### Llama 3.2 90B\n- **RAM**: 64GB minimum, 128GB recommended\n- **GPU**: NVIDIA RTX 4090 (24GB VRAM) or better\n- **Storage**: 50GB free space\n- **CPU**: 16+ cores recommended\n- **Best for**: Production deployments, highest quality\n\n### Llama 3.2 11B\n- **RAM**: 16GB minimum, 32GB recommended\n- **GPU**: NVIDIA RTX 3060 (12GB VRAM) or better\n- **Storage**: 10GB free space\n- **CPU**: 8+ cores recommended\n- **Best for**: Development, good balance\n\n### Llama 3.2 3B\n- **RAM**: 8GB minimum, 16GB recommended\n- **GPU**: NVIDIA GTX 1660 (6GB VRAM) or better\n- **Storage**: 5GB free space\n- **CPU**: 4+ cores recommended\n- **Best for**: Testing, fast responses\n\n### Llama 3.2 1B\n- **RAM**: 4GB minimum, 8GB recommended\n- **GPU**: Optional (runs on CPU)\n- **Storage**: 2GB free space\n- **CPU**: 2+ cores minimum\n- **Best for**: Ultra-fast, minimal resources\n\n## Performance Optimization\n\n### GPU Acceleration (NVIDIA)\n\n1. **Install CUDA**:\n```bash\n# Ubuntu/Debian\nsudo apt install nvidia-cuda-toolkit\n\n# Verify\nnvidia-smi\n```\n\n2. **Configure Ollama for GPU**:\n```bash\n# Ollama automatically uses GPU if available\nollama run llama3.2:90b\n```\n\n3. **Monitor GPU usage**:\n```bash\nwatch -n 1 nvidia-smi\n```\n\n### CPU Optimization\n\n1. **Set thread count**:\n```bash\nexport OLLAMA_NUM_THREADS=16\nollama serve\n```\n\n2. **Increase context size**:\n```bash\nexport OLLAMA_MAX_LOADED_MODELS=2\nexport OLLAMA_NUM_PARALLEL=4\nollama serve\n```\n\n### Memory Management\n\n1. **Limit model memory**:\n```bash\n# Keep model in memory\nexport OLLAMA_KEEP_ALIVE=24h\n\n# Unload after 5 minutes\nexport OLLAMA_KEEP_ALIVE=5m\n```\n\n2. **Preload models**:\n```bash\nollama run llama3.2:90b "test"\n```\n\n## Testing Your Setup\n\n### 1. Test Ollama Directly\n```bash\ncurl http://localhost:11434/api/generate -d \'{\n  "model": "llama3.2:90b",\n  "prompt": "Explain what HIPAA compliance means.",\n  "stream": false\n}\'\n```\n\n### 2. Test Open WebUI\n```bash\ncurl http://localhost:3000/api/chat/completions \\\n  -H "Content-Type: application/json" \\\n  -d \'{\n    "model": "llama3.2:90b",\n    "messages": [\n      {"role": "user", "content": "What is HIPAA?"}\n    ]\n  }\'\n```\n\n### 3. Test HoloVitals Integration\n```typescript\nimport { getProviderManager } from \'@/lib/providers/ProviderManager\';\n\nconst manager = getProviderManager();\nmanager.switchProvider(\'llama-90b\');\n\nconst response = await manager.complete({\n  messages: [\n    { role: \'user\', content: \'Analyze this medical report...\' }\n  ],\n  model: AIModel.LLAMA_32_90B\n});\n\nconsole.log(response.content);\n```\n\n## Production Deployment\n\n### Docker Swarm (Multi-Node)\n\n```yaml\nversion: \'3.8\'\n\nservices:\n  ollama:\n    image: ollama/ollama:latest\n    deploy:\n      replicas: 3\n      resources:\n        limits:\n          cpus: \'16\'\n          memory: 64G\n        reservations:\n          devices:\n            - driver: nvidia\n              count: 1\n              capabilities: [gpu]\n    ports:\n      - "11434:11434"\n    volumes:\n      - ollama_data:/root/.ollama\n\n  open-webui:\n    image: ghcr.io/open-webui/open-webui:main\n    deploy:\n      replicas: 2\n    ports:\n      - "3000:8080"\n    environment:\n      - OLLAMA_BASE_URL=http://ollama:11434\n\nvolumes:\n  ollama_data:\n```\n\n### Kubernetes\n\n```yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: ollama\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: ollama\n  template:\n    metadata:\n      labels:\n        app: ollama\n    spec:\n      containers:\n      - name: ollama\n        image: ollama/ollama:latest\n        ports:\n        - containerPort: 11434\n        resources:\n          limits:\n            nvidia.com/gpu: 1\n            memory: 64Gi\n            cpu: 16\n        volumeMounts:\n        - name: ollama-data\n          mountPath: /root/.ollama\n      volumes:\n      - name: ollama-data\n        persistentVolumeClaim:\n          claimName: ollama-pvc\n```\n\n## Monitoring & Maintenance\n\n### Health Checks\n```bash\n# Check Ollama status\ncurl http://localhost:11434/api/tags\n\n# Check Open WebUI status\ncurl http://localhost:3000/api/health\n\n# Check model list\nollama list\n```\n\n### Logs\n```bash\n# Ollama logs\ndocker logs ollama -f\n\n# Open WebUI logs\ndocker logs open-webui -f\n```\n\n### Updates\n```bash\n# Update Ollama\ncurl -fsSL https://ollama.com/install.sh | sh\n\n# Update models\nollama pull llama3.2:90b\n\n# Update Open WebUI\ndocker pull ghcr.io/open-webui/open-webui:main\ndocker-compose up -d\n```\n\n## Troubleshooting\n\n### Issue: Model not loading\n```bash\n# Check available memory\nfree -h\n\n# Check GPU memory\nnvidia-smi\n\n# Try smaller model\nollama pull llama3.2:3b\n```\n\n### Issue: Slow responses\n```bash\n# Enable GPU\nexport CUDA_VISIBLE_DEVICES=0\nollama serve\n\n# Increase threads\nexport OLLAMA_NUM_THREADS=16\nollama serve\n\n# Preload model\nollama run llama3.2:90b "test"\n```\n\n### Issue: Connection refused\n```bash\n# Check if Ollama is running\nps aux | grep ollama\n\n# Restart Ollama\npkill ollama\nollama serve\n\n# Check port\nnetstat -tulpn | grep 11434\n```\n\n### Issue: Out of memory\n```bash\n# Use smaller model\nollama pull llama3.2:3b\n\n# Reduce context size\nexport OLLAMA_MAX_LOADED_MODELS=1\n\n# Clear cache\nrm -rf ~/.ollama/models\n```\n\n## Security Best Practices\n\n1. **Network Security**:\n```bash\n# Bind to localhost only\nexport OLLAMA_HOST=127.0.0.1:11434\nollama serve\n```\n\n2. **Firewall Rules**:\n```bash\n# Allow only local connections\nsudo ufw allow from 127.0.0.1 to any port 11434\n```\n\n3. **Access Control**:\n```bash\n# Use reverse proxy with authentication\n# nginx.conf\nlocation /api/ {\n    auth_basic "Restricted";\n    auth_basic_user_file /etc/nginx/.htpasswd;\n    proxy_pass http://localhost:11434/;\n}\n```\n\n4. **Data Encryption**:\n```bash\n# Encrypt model storage\ncryptsetup luksFormat /dev/sdb1\ncryptsetup open /dev/sdb1 ollama_encrypted\nmkfs.ext4 /dev/mapper/ollama_encrypted\nmount /dev/mapper/ollama_encrypted ~/.ollama\n```\n\n## Cost Savings Calculator\n\n### Cloud vs Local (Annual)\n\n**Cloud (GPT-4)**:\n- 1M tokens/day Ã— 365 days = 365M tokens/year\n- Cost: 365M Ã— $30/1M = $10,950/year\n\n**Local (Llama 3.2 90B)**:\n- Hardware: $5,000 (one-time)\n- Electricity: ~$500/year (24/7 operation)\n- Total Year 1: $5,500\n- Total Year 2+: $500/year\n\n**Savings**: $5,450 in Year 1, $10,450/year after\n\n## Advanced Configuration\n\n### Custom Model Parameters\n```bash\n# Create Modelfile\ncat > Modelfile <<EOF\nFROM llama3.2:90b\nPARAMETER temperature 0.7\nPARAMETER top_p 0.9\nPARAMETER top_k 40\nPARAMETER num_ctx 128000\nSYSTEM You are a medical AI assistant specialized in analyzing patient records.\nEOF\n\n# Create custom model\nollama create medical-llama -f Modelfile\n```\n\n### Fine-Tuning (Advanced)\n```bash\n# Prepare training data\n# train.jsonl format:\n# {"prompt": "...", "completion": "..."}\n\n# Fine-tune (requires additional tools)\n# See: https://github.com/ollama/ollama/blob/main/docs/faq.md#how-do-i-fine-tune-a-model\n```\n\n## Support & Resources\n\n- **Ollama Docs**: https://github.com/ollama/ollama\n- **Open WebUI Docs**: https://docs.openwebui.com\n- **Llama Models**: https://ai.meta.com/llama\n- **Community**: https://discord.gg/ollama\n- **Issues**: https://github.com/cloudbyday90/HoloVitals/issues\n\n## Next Steps\n\n1. âœ… Install Ollama and Open WebUI\n2. âœ… Pull Llama models\n3. âœ… Configure HoloVitals\n4. âœ… Test integration\n5. âœ… Monitor performance\n6. âœ… Optimize for your workload\n7. âœ… Deploy to production\n\nHappy local LLM deployment! ðŸš€\n</create-file>\n</function_calls>'}