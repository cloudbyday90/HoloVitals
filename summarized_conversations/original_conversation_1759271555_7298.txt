{'role': 'assistant', 'content': 'Now let me create a quick setup guide for local Llama deployment:\n\n<function_calls>\n<create-file file_path="medical-analysis-platform/docs/LLAMA_SETUP_GUIDE.md">\n# Llama Local LLM Setup Guide\n\n## Why Use Local Llama Models?\n\n### Benefits\n- ✅ **FREE** - No API costs, unlimited usage\n- ✅ **PRIVATE** - Data never leaves your infrastructure\n- ✅ **OFFLINE** - Works without internet connection\n- ✅ **HIPAA-COMPLIANT** - Complete data control\n- ✅ **FAST** - No network latency\n- ✅ **CUSTOMIZABLE** - Fine-tune for your needs\n\n### Use Cases\n- Medical data analysis (HIPAA compliance)\n- Development and testing\n- High-volume processing\n- Sensitive patient information\n- Offline deployments\n- Cost optimization\n\n## Quick Start (5 Minutes)\n\n### Option 1: Docker Compose (Recommended)\n\n1. **Create docker-compose.yml**:\n```yaml\nversion: \'3.8\'\n\nservices:\n  ollama:\n    image: ollama/ollama:latest\n    container_name: ollama\n    ports:\n      - "11434:11434"\n    volumes:\n      - ollama_data:/root/.ollama\n    restart: unless-stopped\n\n  open-webui:\n    image: ghcr.io/open-webui/open-webui:main\n    container_name: open-webui\n    ports:\n      - "3000:8080"\n    environment:\n      - OLLAMA_BASE_URL=http://ollama:11434\n    volumes:\n      - open-webui_data:/app/backend/data\n    depends_on:\n      - ollama\n    restart: unless-stopped\n\nvolumes:\n  ollama_data:\n  open-webui_data:\n```\n\n2. **Start services**:\n```bash\ndocker-compose up -d\n```\n\n3. **Pull Llama models**:\n```bash\ndocker exec -it ollama ollama pull llama3.2:90b\ndocker exec -it ollama ollama pull llama3.2:11b\ndocker exec -it ollama ollama pull llama3.2:3b\ndocker exec -it ollama ollama pull llama3.2:1b\n```\n\n4. **Configure HoloVitals** - Add to `.env`:\n```bash\nOPEN_WEBUI_URL="http://localhost:3000/api"\n```\n\n5. **Test**:\n```bash\ncurl http://localhost:3000/api/health\n```\n\n### Option 2: Native Installation\n\n#### Step 1: Install Ollama\n\n**Linux/Mac**:\n```bash\ncurl -fsSL https://ollama.com/install.sh | sh\n```\n\n**Windows**:\nDownload from: https://ollama.com/download/windows\n\n#### Step 2: Start Ollama\n```bash\nollama serve\n```\n\n#### Step 3: Pull Models\n```bash\n# Large model (best quality, requires 64GB RAM)\nollama pull llama3.2:90b\n\n# Medium model (good balance, requires 16GB RAM)\nollama pull llama3.2:11b\n\n# Small model (fast, requires 4GB RAM)\nollama pull llama3.2:3b\n\n# Tiny model (ultra-fast, requires 2GB RAM)\nollama pull llama3.2:1b\n```\n\n#### Step 4: Install Open WebUI (Optional)\n```bash\ndocker run -d -p 3000:8080 \\\n  -e OLLAMA_BASE_URL=http://host.docker.internal:11434 \\\n  -v open-webui:/app/backend/data \\\n  --name open-webui \\\n  ghcr.io/open-webui/open-webui:main\n```\n\n#### Step 5: Configure HoloVitals\nAdd to `.env`:\n```bash\nLLAMA_BASE_URL="http://localhost:11434/api"\n# OR (if using Open WebUI)\nOPEN_WEBUI_URL="http://localhost:3000/api"\n```\n\n## System Requirements\n\n### Llama 3.2 90B\n- **RAM**: 64GB minimum, 128GB recommended\n- **GPU**: NVIDIA RTX 4090 (24GB VRAM) or better\n- **Storage**: 50GB free space\n- **CPU**: 16+ cores recommended\n- **Best for**: Production deployments, highest quality\n\n### Llama 3.2 11B\n- **RAM**: 16GB minimum, 32GB recommended\n- **GPU**: NVIDIA RTX 3060 (12GB VRAM) or better\n- **Storage**: 10GB free space\n- **CPU**: 8+ cores recommended\n- **Best for**: Development, good balance\n\n### Llama 3.2 3B\n- **RAM**: 8GB minimum, 16GB recommended\n- **GPU**: NVIDIA GTX 1660 (6GB VRAM) or better\n- **Storage**: 5GB free space\n- **CPU**: 4+ cores recommended\n- **Best for**: Testing, fast responses\n\n### Llama 3.2 1B\n- **RAM**: 4GB minimum, 8GB recommended\n- **GPU**: Optional (runs on CPU)\n- **Storage**: 2GB free space\n- **CPU**: 2+ cores minimum\n- **Best for**: Ultra-fast, minimal resources\n\n## Performance Optimization\n\n### GPU Acceleration (NVIDIA)\n\n1. **Install CUDA**:\n```bash\n# Ubuntu/Debian\nsudo apt install nvidia-cuda-toolkit\n\n# Verify\nnvidia-smi\n```\n\n2. **Configure Ollama for GPU**:\n```bash\n# Ollama automatically uses GPU if available\nollama run llama3.2:90b\n```\n\n3. **Monitor GPU usage**:\n```bash\nwatch -n 1 nvidia-smi\n```\n\n### CPU Optimization\n\n1. **Set thread count**:\n```bash\nexport OLLAMA_NUM_THREADS=16\nollama serve\n```\n\n2. **Increase context size**:\n```bash\nexport OLLAMA_MAX_LOADED_MODELS=2\nexport OLLAMA_NUM_PARALLEL=4\nollama serve\n```\n\n### Memory Management\n\n1. **Limit model memory**:\n```bash\n# Keep model in memory\nexport OLLAMA_KEEP_ALIVE=24h\n\n# Unload after 5 minutes\nexport OLLAMA_KEEP_ALIVE=5m\n```\n\n2. **Preload models**:\n```bash\nollama run llama3.2:90b "test"\n```\n\n## Testing Your Setup\n\n### 1. Test Ollama Directly\n```bash\ncurl http://localhost:11434/api/generate -d \'{\n  "model": "llama3.2:90b",\n  "prompt": "Explain what HIPAA compliance means.",\n  "stream": false\n}\'\n```\n\n### 2. Test Open WebUI\n```bash\ncurl http://localhost:3000/api/chat/completions \\\n  -H "Content-Type: application/json" \\\n  -d \'{\n    "model": "llama3.2:90b",\n    "messages": [\n      {"role": "user", "content": "What is HIPAA?"}\n    ]\n  }\'\n```\n\n### 3. Test HoloVitals Integration\n```typescript\nimport { getProviderManager } from \'@/lib/providers/ProviderManager\';\n\nconst manager = getProviderManager();\nmanager.switchProvider(\'llama-90b\');\n\nconst response = await manager.complete({\n  messages: [\n    { role: \'user\', content: \'Analyze this medical report...\' }\n  ],\n  model: AIModel.LLAMA_32_90B\n});\n\nconsole.log(response.content);\n```\n\n## Production Deployment\n\n### Docker Swarm (Multi-Node)\n\n```yaml\nversion: \'3.8\'\n\nservices:\n  ollama:\n    image: ollama/ollama:latest\n    deploy:\n      replicas: 3\n      resources:\n        limits:\n          cpus: \'16\'\n          memory: 64G\n        reservations:\n          devices:\n            - driver: nvidia\n              count: 1\n              capabilities: [gpu]\n    ports:\n      - "11434:11434"\n    volumes:\n      - ollama_data:/root/.ollama\n\n  open-webui:\n    image: ghcr.io/open-webui/open-webui:main\n    deploy:\n      replicas: 2\n    ports:\n      - "3000:8080"\n    environment:\n      - OLLAMA_BASE_URL=http://ollama:11434\n\nvolumes:\n  ollama_data:\n```\n\n### Kubernetes\n\n```yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: ollama\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: ollama\n  template:\n    metadata:\n      labels:\n        app: ollama\n    spec:\n      containers:\n      - name: ollama\n        image: ollama/ollama:latest\n        ports:\n        - containerPort: 11434\n        resources:\n          limits:\n            nvidia.com/gpu: 1\n            memory: 64Gi\n            cpu: 16\n        volumeMounts:\n        - name: ollama-data\n          mountPath: /root/.ollama\n      volumes:\n      - name: ollama-data\n        persistentVolumeClaim:\n          claimName: ollama-pvc\n```\n\n## Monitoring & Maintenance\n\n### Health Checks\n```bash\n# Check Ollama status\ncurl http://localhost:11434/api/tags\n\n# Check Open WebUI status\ncurl http://localhost:3000/api/health\n\n# Check model list\nollama list\n```\n\n### Logs\n```bash\n# Ollama logs\ndocker logs ollama -f\n\n# Open WebUI logs\ndocker logs open-webui -f\n```\n\n### Updates\n```bash\n# Update Ollama\ncurl -fsSL https://ollama.com/install.sh | sh\n\n# Update models\nollama pull llama3.2:90b\n\n# Update Open WebUI\ndocker pull ghcr.io/open-webui/open-webui:main\ndocker-compose up -d\n```\n\n## Troubleshooting\n\n### Issue: Model not loading\n```bash\n# Check available memory\nfree -h\n\n# Check GPU memory\nnvidia-smi\n\n# Try smaller model\nollama pull llama3.2:3b\n```\n\n### Issue: Slow responses\n```bash\n# Enable GPU\nexport CUDA_VISIBLE_DEVICES=0\nollama serve\n\n# Increase threads\nexport OLLAMA_NUM_THREADS=16\nollama serve\n\n# Preload model\nollama run llama3.2:90b "test"\n```\n\n### Issue: Connection refused\n```bash\n# Check if Ollama is running\nps aux | grep ollama\n\n# Restart Ollama\npkill ollama\nollama serve\n\n# Check port\nnetstat -tulpn | grep 11434\n```\n\n### Issue: Out of memory\n```bash\n# Use smaller model\nollama pull llama3.2:3b\n\n# Reduce context size\nexport OLLAMA_MAX_LOADED_MODELS=1\n\n# Clear cache\nrm -rf ~/.ollama/models\n```\n\n## Security Best Practices\n\n1. **Network Security**:\n```bash\n# Bind to localhost only\nexport OLLAMA_HOST=127.0.0.1:11434\nollama serve\n```\n\n2. **Firewall Rules**:\n```bash\n# Allow only local connections\nsudo ufw allow from 127.0.0.1 to any port 11434\n```\n\n3. **Access Control**:\n```bash\n# Use reverse proxy with authentication\n# nginx.conf\nlocation /api/ {\n    auth_basic "Restricted";\n    auth_basic_user_file /etc/nginx/.htpasswd;\n    proxy_pass http://localhost:11434/;\n}\n```\n\n4. **Data Encryption**:\n```bash\n# Encrypt model storage\ncryptsetup luksFormat /dev/sdb1\ncryptsetup open /dev/sdb1 ollama_encrypted\nmkfs.ext4 /dev/mapper/ollama_encrypted\nmount /dev/mapper/ollama_encrypted ~/.ollama\n```\n\n## Cost Savings Calculator\n\n### Cloud vs Local (Annual)\n\n**Cloud (GPT-4)**:\n- 1M tokens/day × 365 days = 365M tokens/year\n- Cost: 365M × $30/1M = $10,950/year\n\n**Local (Llama 3.2 90B)**:\n- Hardware: $5,000 (one-time)\n- Electricity: ~$500/year (24/7 operation)\n- Total Year 1: $5,500\n- Total Year 2+: $500/year\n\n**Savings**: $5,450 in Year 1, $10,450/year after\n\n## Advanced Configuration\n\n### Custom Model Parameters\n```bash\n# Create Modelfile\ncat > Modelfile <<EOF\nFROM llama3.2:90b\nPARAMETER temperature 0.7\nPARAMETER top_p 0.9\nPARAMETER top_k 40\nPARAMETER num_ctx 128000\nSYSTEM You are a medical AI assistant specialized in analyzing patient records.\nEOF\n\n# Create custom model\nollama create medical-llama -f Modelfile\n```\n\n### Fine-Tuning (Advanced)\n```bash\n# Prepare training data\n# train.jsonl format:\n# {"prompt": "...", "completion": "..."}\n\n# Fine-tune (requires additional tools)\n# See: https://github.com/ollama/ollama/blob/main/docs/faq.md#how-do-i-fine-tune-a-model\n```\n\n## Support & Resources\n\n- **Ollama Docs**: https://github.com/ollama/ollama\n- **Open WebUI Docs**: https://docs.openwebui.com\n- **Llama Models**: https://ai.meta.com/llama\n- **Community**: https://discord.gg/ollama\n- **Issues**: https://github.com/cloudbyday90/HoloVitals/issues\n\n## Next Steps\n\n1. ✅ Install Ollama and Open WebUI\n2. ✅ Pull Llama models\n3. ✅ Configure HoloVitals\n4. ✅ Test integration\n5. ✅ Monitor performance\n6. ✅ Optimize for your workload\n7. ✅ Deploy to production\n\nHappy local LLM deployment! 🚀\n</create-file>\n</function_calls>'}