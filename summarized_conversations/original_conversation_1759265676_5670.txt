{'role': 'assistant', 'content': 'Now let\'s create utility functions for OpenAI integration and token counting:\n\n<function_calls>\n<create-file file_path="medical-analysis-platform/lib/utils/openai.ts">\n/**\n * OpenAI Client Wrapper\n * \n * Provides a configured OpenAI client with error handling,\n * retry logic, and cost tracking.\n */\n\nimport OpenAI from \'openai\';\nimport { TOKEN_COSTS } from \'@/lib/types/chatbot\';\n\n// Initialize OpenAI client\nexport const openai = new OpenAI({\n  apiKey: process.env.OPENAI_API_KEY,\n});\n\n/**\n * Calculate cost for token usage\n */\nexport function calculateCost(\n  model: string,\n  promptTokens: number,\n  completionTokens: number\n): number {\n  const costs = TOKEN_COSTS[model as keyof typeof TOKEN_COSTS];\n  \n  if (!costs) {\n    console.warn(`Unknown model: ${model}, using gpt-3.5-turbo costs`);\n    const defaultCosts = TOKEN_COSTS[\'gpt-3.5-turbo\'];\n    return (\n      (promptTokens / 1000) * defaultCosts.prompt +\n      (completionTokens / 1000) * defaultCosts.completion\n    );\n  }\n\n  return (\n    (promptTokens / 1000) * costs.prompt +\n    (completionTokens / 1000) * costs.completion\n  );\n}\n\n/**\n * Create chat completion with error handling\n */\nexport async function createChatCompletion(\n  messages: Array<{ role: string; content: string }>,\n  options: {\n    model?: string;\n    maxTokens?: number;\n    temperature?: number;\n    topP?: number;\n    frequencyPenalty?: number;\n    presencePenalty?: number;\n    stream?: boolean;\n  } = {}\n) {\n  const {\n    model = \'gpt-3.5-turbo\',\n    maxTokens = 500,\n    temperature = 0.7,\n    topP = 1,\n    frequencyPenalty = 0,\n    presencePenalty = 0,\n    stream = false\n  } = options;\n\n  try {\n    const response = await openai.chat.completions.create({\n      model,\n      messages: messages as any,\n      max_tokens: maxTokens,\n      temperature,\n      top_p: topP,\n      frequency_penalty: frequencyPenalty,\n      presence_penalty: presencePenalty,\n      stream\n    });\n\n    return response;\n  } catch (error: any) {\n    console.error(\'OpenAI API Error:\', error);\n    \n    // Handle specific error types\n    if (error.status === 429) {\n      throw new Error(\'Rate limit exceeded. Please try again later.\');\n    } else if (error.status === 401) {\n      throw new Error(\'Invalid API key. Please check your configuration.\');\n    } else if (error.status === 500) {\n      throw new Error(\'OpenAI service error. Please try again later.\');\n    }\n    \n    throw new Error(`OpenAI API error: ${error.message}`);\n  }\n}\n\n/**\n * Create streaming chat completion\n */\nexport async function* createStreamingChatCompletion(\n  messages: Array<{ role: string; content: string }>,\n  options: {\n    model?: string;\n    maxTokens?: number;\n    temperature?: number;\n    topP?: number;\n    frequencyPenalty?: number;\n    presencePenalty?: number;\n  } = {}\n) {\n  const {\n    model = \'gpt-3.5-turbo\',\n    maxTokens = 500,\n    temperature = 0.7,\n    topP = 1,\n    frequencyPenalty = 0,\n    presencePenalty = 0\n  } = options;\n\n  try {\n    const stream = await openai.chat.completions.create({\n      model,\n      messages: messages as any,\n      max_tokens: maxTokens,\n      temperature,\n      top_p: topP,\n      frequency_penalty: frequencyPenalty,\n      presence_penalty: presencePenalty,\n      stream: true\n    });\n\n    for await (const chunk of stream) {\n      const content = chunk.choices[0]?.delta?.content;\n      if (content) {\n        yield content;\n      }\n    }\n  } catch (error: any) {\n    console.error(\'OpenAI Streaming Error:\', error);\n    throw new Error(`Streaming error: ${error.message}`);\n  }\n}\n\n/**\n * Retry wrapper for API calls\n */\nexport async function withRetry<T>(\n  fn: () => Promise<T>,\n  maxRetries: number = 3,\n  delayMs: number = 1000\n): Promise<T> {\n  let lastError: Error;\n\n  for (let i = 0; i < maxRetries; i++) {\n    try {\n      return await fn();\n    } catch (error: any) {\n      lastError = error;\n      \n      // Don\'t retry on authentication errors\n      if (error.status === 401) {\n        throw error;\n      }\n\n      // Wait before retrying\n      if (i < maxRetries - 1) {\n        await new Promise(resolve => setTimeout(resolve, delayMs * (i + 1)));\n      }\n    }\n  }\n\n  throw lastError!;\n}\n\n/**\n * Validate OpenAI API key\n */\nexport async function validateApiKey(): Promise<boolean> {\n  try {\n    await openai.models.list();\n    return true;\n  } catch (error) {\n    console.error(\'Invalid OpenAI API key\');\n    return false;\n  }\n}\n\n/**\n * Get available models\n */\nexport async function getAvailableModels(): Promise<string[]> {\n  try {\n    const response = await openai.models.list();\n    return response.data\n      .filter(model => model.id.includes(\'gpt\'))\n      .map(model => model.id);\n  } catch (error) {\n    console.error(\'Error fetching models:\', error);\n    return [\'gpt-3.5-turbo\', \'gpt-4\'];\n  }\n}\n</create-file>\n</function_calls>'}