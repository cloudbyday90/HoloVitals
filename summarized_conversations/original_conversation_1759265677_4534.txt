{'role': 'assistant', 'content': 'Now let\'s create the token counter utility:\n\n<function_calls>\n<create-file file_path="medical-analysis-platform/lib/utils/tokenCounter.ts">\n/**\n * Token Counter Utility\n * \n * Provides accurate token counting for OpenAI models using tiktoken.\n * Essential for cost estimation and context window management.\n */\n\nimport { encoding_for_model, TiktokenModel } from \'tiktoken\';\n\n// Cache encodings to avoid repeated initialization\nconst encodingCache = new Map<string, any>();\n\n/**\n * Get encoding for a specific model\n */\nfunction getEncoding(model: string) {\n  if (encodingCache.has(model)) {\n    return encodingCache.get(model);\n  }\n\n  try {\n    const encoding = encoding_for_model(model as TiktokenModel);\n    encodingCache.set(model, encoding);\n    return encoding;\n  } catch (error) {\n    // Fallback to gpt-3.5-turbo encoding\n    console.warn(`Unknown model ${model}, using gpt-3.5-turbo encoding`);\n    const encoding = encoding_for_model(\'gpt-3.5-turbo\');\n    encodingCache.set(model, encoding);\n    return encoding;\n  }\n}\n\n/**\n * Count tokens in a text string\n */\nexport function countTokens(text: string, model: string = \'gpt-3.5-turbo\'): number {\n  if (!text || text.length === 0) {\n    return 0;\n  }\n\n  try {\n    const encoding = getEncoding(model);\n    const tokens = encoding.encode(text);\n    return tokens.length;\n  } catch (error) {\n    console.error(\'Error counting tokens:\', error);\n    // Fallback: rough estimate (1 token â‰ˆ 4 characters)\n    return Math.ceil(text.length / 4);\n  }\n}\n\n/**\n * Count tokens in a message array\n */\nexport function countMessageTokens(\n  messages: Array<{ role: string; content: string }>,\n  model: string = \'gpt-3.5-turbo\'\n): number {\n  let totalTokens = 0;\n\n  // Add tokens for each message\n  for (const message of messages) {\n    // Every message follows <|start|>{role/name}\\n{content}<|end|>\\n\n    totalTokens += 4; // Message formatting tokens\n    totalTokens += countTokens(message.role, model);\n    totalTokens += countTokens(message.content, model);\n  }\n\n  // Add tokens for reply priming\n  totalTokens += 3;\n\n  return totalTokens;\n}\n\n/**\n * Estimate tokens for a chat completion\n */\nexport function estimateCompletionTokens(\n  promptTokens: number,\n  maxTokens: number = 500\n): number {\n  // Estimate based on typical completion length\n  // Usually 20-40% of max tokens for conversational responses\n  return Math.min(maxTokens, Math.ceil(promptTokens * 0.3));\n}\n\n/**\n * Check if messages fit within context window\n */\nexport function fitsInContextWindow(\n  messages: Array<{ role: string; content: string }>,\n  maxContextTokens: number,\n  model: string = \'gpt-3.5-turbo\'\n): boolean {\n  const totalTokens = countMessageTokens(messages, model);\n  return totalTokens <= maxContextTokens;\n}\n\n/**\n * Truncate messages to fit within context window\n */\nexport function truncateMessages(\n  messages: Array<{ role: string; content: string }>,\n  maxContextTokens: number,\n  model: string = \'gpt-3.5-turbo\',\n  keepSystemMessage: boolean = true\n): Array<{ role: string; content: string }> {\n  if (messages.length === 0) {\n    return [];\n  }\n\n  // Always keep system message if requested\n  const systemMessage = keepSystemMessage && messages[0]?.role === \'system\' \n    ? [messages[0]] \n    : [];\n  \n  const otherMessages = keepSystemMessage && messages[0]?.role === \'system\'\n    ? messages.slice(1)\n    : messages;\n\n  // Calculate tokens for system message\n  const systemTokens = systemMessage.length > 0 \n    ? countMessageTokens(systemMessage, model) \n    : 0;\n\n  const availableTokens = maxContextTokens - systemTokens;\n\n  // Start from the most recent messages and work backwards\n  const truncated: Array<{ role: string; content: string }> = [];\n  let currentTokens = 0;\n\n  for (let i = otherMessages.length - 1; i >= 0; i--) {\n    const message = otherMessages[i];\n    const messageTokens = countMessageTokens([message], model);\n\n    if (currentTokens + messageTokens <= availableTokens) {\n      truncated.unshift(message);\n      currentTokens += messageTokens;\n    } else {\n      break;\n    }\n  }\n\n  return [...systemMessage, ...truncated];\n}\n\n/**\n * Get context window size for a model\n */\nexport function getContextWindowSize(model: string): number {\n  const contextWindows: Record<string, number> = {\n    \'gpt-3.5-turbo\': 4096,\n    \'gpt-3.5-turbo-16k\': 16384,\n    \'gpt-4\': 8192,\n    \'gpt-4-32k\': 32768,\n    \'gpt-4-turbo\': 128000,\n    \'gpt-4-turbo-preview\': 128000\n  };\n\n  return contextWindows[model] || 4096;\n}\n\n/**\n * Calculate available tokens for completion\n */\nexport function getAvailableCompletionTokens(\n  messages: Array<{ role: string; content: string }>,\n  model: string = \'gpt-3.5-turbo\',\n  maxCompletionTokens: number = 500\n): number {\n  const contextWindow = getContextWindowSize(model);\n  const promptTokens = countMessageTokens(messages, model);\n  const available = contextWindow - promptTokens;\n\n  return Math.min(available, maxCompletionTokens);\n}\n\n/**\n * Optimize message content by removing unnecessary whitespace\n */\nexport function optimizeMessageContent(content: string): string {\n  return content\n    .replace(/\\s+/g, \' \')           // Replace multiple spaces with single space\n    .replace(/\\n\\s*\\n/g, \'\\n\')      // Remove empty lines\n    .trim();                         // Remove leading/trailing whitespace\n}\n\n/**\n * Get token statistics for messages\n */\nexport function getTokenStatistics(\n  messages: Array<{ role: string; content: string }>,\n  model: string = \'gpt-3.5-turbo\'\n): {\n  totalTokens: number;\n  messageCount: number;\n  averageTokensPerMessage: number;\n  contextWindow: number;\n  utilizationPercentage: number;\n} {\n  const totalTokens = countMessageTokens(messages, model);\n  const messageCount = messages.length;\n  const averageTokensPerMessage = messageCount > 0 ? totalTokens / messageCount : 0;\n  const contextWindow = getContextWindowSize(model);\n  const utilizationPercentage = (totalTokens / contextWindow) * 100;\n\n  return {\n    totalTokens,\n    messageCount,\n    averageTokensPerMessage,\n    contextWindow,\n    utilizationPercentage\n  };\n}\n\n/**\n * Cleanup: Free encoding resources\n */\nexport function cleanup() {\n  for (const [, encoding] of encodingCache) {\n    encoding.free();\n  }\n  encodingCache.clear();\n}\n</create-file>\n</function_calls>'}