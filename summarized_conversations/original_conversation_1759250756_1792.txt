{'role': 'assistant', 'content': '<function_calls>\n<create-file file_path="ARCHITECTURE.md">\n# System Architecture - Medical Analysis Platform\n\n## High-Level Architecture\n\n### 1. Frontend Layer (Next.js 14+)\n\n```typescript\napp/\n├── (auth)/\n│   ├── login/\n│   └── register/\n├── (dashboard)/\n│   ├── documents/\n│   │   ├── upload/\n│   │   ├── [id]/\n│   │   └── list/\n│   ├── analysis/\n│   │   ├── trends/\n│   │   └── insights/\n│   └── settings/\n├── api/\n│   ├── documents/\n│   ├── ocr/\n│   ├── analysis/\n│   └── auth/\n└── components/\n    ├── document-viewer/\n    ├── upload-zone/\n    ├── analysis-panel/\n    └── ui/\n```\n\n### 2. Backend Services Architecture\n\n#### Document Service\n**Responsibilities:**\n- Handle document uploads\n- Manage document metadata\n- Organize documents by type and patient\n- Provide document retrieval APIs\n\n**Key Components:**\n```typescript\ninterface DocumentService {\n  uploadDocument(file: File, metadata: DocumentMetadata): Promise<Document>\n  getDocument(id: string): Promise<Document>\n  listDocuments(filters: DocumentFilters): Promise<Document[]>\n  deleteDocument(id: string): Promise<void>\n  linkDocuments(sourceId: string, targetId: string): Promise<void>\n}\n```\n\n#### OCR Service\n**Responsibilities:**\n- Extract text from PDFs\n- Identify document structure\n- Extract structured data (dates, values, test names)\n- Handle different medical document formats\n\n**Processing Pipeline:**\n```\nPDF Upload → Text Extraction → Structure Detection → Data Parsing → Storage\n```\n\n**Key Components:**\n```typescript\ninterface OCRService {\n  extractText(documentId: string): Promise<ExtractedText>\n  parseStructuredData(text: string, documentType: DocumentType): Promise<StructuredData>\n  identifyDocumentType(text: string): Promise<DocumentType>\n}\n\ninterface StructuredData {\n  documentType: \'bloodwork\' | \'imaging\' | \'aftercare\' | \'prescription\'\n  date: Date\n  provider?: string\n  patient?: PatientInfo\n  results?: TestResult[]\n  findings?: string[]\n  recommendations?: string[]\n}\n```\n\n#### AI Agent Service\n**Responsibilities:**\n- Process natural language queries\n- Analyze documents with context\n- Generate insights and recommendations\n- Maintain conversation history\n\n**Context Management:**\n```typescript\ninterface ContextManager {\n  // Build context from related documents\n  buildContext(documentIds: string[]): Promise<Context>\n  \n  // Maintain session context\n  updateSessionContext(sessionId: string, newInfo: any): Promise<void>\n  \n  // Retrieve relevant historical data\n  getRelevantHistory(query: string, patientId: string): Promise<Document[]>\n}\n\ninterface AIAgent {\n  query(question: string, context: Context): Promise<AIResponse>\n  analyzeDocument(documentId: string, context: Context): Promise<Analysis>\n  compareTrends(documentIds: string[]): Promise<TrendAnalysis>\n  generateInsights(patientId: string): Promise<Insight[]>\n}\n```\n\n### 3. Database Schema\n\n```sql\n-- Users and Authentication\nCREATE TABLE users (\n  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),\n  email VARCHAR(255) UNIQUE NOT NULL,\n  password_hash VARCHAR(255) NOT NULL,\n  created_at TIMESTAMP DEFAULT NOW(),\n  updated_at TIMESTAMP DEFAULT NOW()\n);\n\n-- Patients (if multi-patient support)\nCREATE TABLE patients (\n  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),\n  user_id UUID REFERENCES users(id),\n  first_name VARCHAR(100),\n  last_name VARCHAR(100),\n  date_of_birth DATE,\n  created_at TIMESTAMP DEFAULT NOW()\n);\n\n-- Documents\nCREATE TABLE documents (\n  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),\n  user_id UUID REFERENCES users(id),\n  patient_id UUID REFERENCES patients(id),\n  file_path VARCHAR(500) NOT NULL,\n  file_name VARCHAR(255) NOT NULL,\n  file_size INTEGER,\n  mime_type VARCHAR(100),\n  document_type VARCHAR(50), -- \'bloodwork\', \'imaging\', \'aftercare\', etc.\n  upload_date TIMESTAMP DEFAULT NOW(),\n  document_date DATE, -- Date of the actual medical document\n  status VARCHAR(50) DEFAULT \'pending\', -- \'pending\', \'processing\', \'completed\', \'failed\'\n  created_at TIMESTAMP DEFAULT NOW(),\n  updated_at TIMESTAMP DEFAULT NOW()\n);\n\n-- OCR Results\nCREATE TABLE ocr_results (\n  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),\n  document_id UUID REFERENCES documents(id) ON DELETE CASCADE,\n  raw_text TEXT,\n  confidence_score DECIMAL(5,2),\n  processed_at TIMESTAMP DEFAULT NOW()\n);\n\n-- Structured Data Extraction\nCREATE TABLE extracted_data (\n  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),\n  document_id UUID REFERENCES documents(id) ON DELETE CASCADE,\n  data_type VARCHAR(50), -- \'test_result\', \'diagnosis\', \'medication\', etc.\n  field_name VARCHAR(100),\n  field_value TEXT,\n  unit VARCHAR(50),\n  reference_range VARCHAR(100),\n  is_abnormal BOOLEAN,\n  extracted_at TIMESTAMP DEFAULT NOW()\n);\n\n-- Document Relationships\nCREATE TABLE document_links (\n  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),\n  source_document_id UUID REFERENCES documents(id) ON DELETE CASCADE,\n  target_document_id UUID REFERENCES documents(id) ON DELETE CASCADE,\n  relationship_type VARCHAR(50), -- \'follow_up\', \'related\', \'supersedes\', etc.\n  created_at TIMESTAMP DEFAULT NOW(),\n  UNIQUE(source_document_id, target_document_id)\n);\n\n-- AI Analysis Sessions\nCREATE TABLE analysis_sessions (\n  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),\n  user_id UUID REFERENCES users(id),\n  patient_id UUID REFERENCES patients(id),\n  session_type VARCHAR(50), -- \'query\', \'trend_analysis\', \'insight_generation\'\n  created_at TIMESTAMP DEFAULT NOW()\n);\n\n-- AI Queries and Responses\nCREATE TABLE ai_interactions (\n  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),\n  session_id UUID REFERENCES analysis_sessions(id) ON DELETE CASCADE,\n  query TEXT NOT NULL,\n  response TEXT,\n  context_documents UUID[], -- Array of document IDs used for context\n  created_at TIMESTAMP DEFAULT NOW()\n);\n\n-- Vector Embeddings for Semantic Search\nCREATE TABLE document_embeddings (\n  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),\n  document_id UUID REFERENCES documents(id) ON DELETE CASCADE,\n  chunk_index INTEGER,\n  chunk_text TEXT,\n  embedding vector(1536), -- OpenAI embedding dimension\n  created_at TIMESTAMP DEFAULT NOW()\n);\n\n-- Indexes for performance\nCREATE INDEX idx_documents_user_id ON documents(user_id);\nCREATE INDEX idx_documents_patient_id ON documents(patient_id);\nCREATE INDEX idx_documents_type ON documents(document_type);\nCREATE INDEX idx_documents_date ON documents(document_date);\nCREATE INDEX idx_extracted_data_document_id ON extracted_data(document_id);\nCREATE INDEX idx_document_links_source ON document_links(source_document_id);\nCREATE INDEX idx_document_links_target ON document_links(target_document_id);\n```\n\n### 4. Context Management Strategy\n\n#### Context Building Process\n\n```typescript\nclass ContextBuilder {\n  async buildAnalysisContext(\n    currentDocumentId: string,\n    userId: string\n  ): Promise<AnalysisContext> {\n    // 1. Get current document and its data\n    const currentDoc = await this.getDocument(currentDocumentId);\n    const currentData = await this.getExtractedData(currentDocumentId);\n    \n    // 2. Find related documents\n    const relatedDocs = await this.findRelatedDocuments(\n      currentDocumentId,\n      currentDoc.documentType,\n      currentDoc.documentDate\n    );\n    \n    // 3. Get historical data for trend analysis\n    const historicalData = await this.getHistoricalData(\n      userId,\n      currentDoc.documentType,\n      currentDoc.documentDate\n    );\n    \n    // 4. Build semantic context using embeddings\n    const semanticContext = await this.getSemanticallySimilar(\n      currentDoc.id,\n      5 // top 5 similar documents\n    );\n    \n    // 5. Compile context\n    return {\n      currentDocument: currentDoc,\n      currentData: currentData,\n      relatedDocuments: relatedDocs,\n      historicalTrends: historicalData,\n      similarDocuments: semanticContext,\n      metadata: {\n        totalDocuments: await this.countUserDocuments(userId),\n        dateRange: await this.getDocumentDateRange(userId),\n        documentTypes: await this.getDocumentTypes(userId)\n      }\n    };\n  }\n  \n  async findRelatedDocuments(\n    documentId: string,\n    documentType: string,\n    documentDate: Date\n  ): Promise<Document[]> {\n    // Find documents that are:\n    // 1. Explicitly linked\n    // 2. Same type within time window\n    // 3. Referenced in the document text\n    \n    const linked = await this.getLinkedDocuments(documentId);\n    const temporal = await this.getTemporallyRelated(\n      documentType,\n      documentDate,\n      30 // days window\n    );\n    \n    return this.deduplicateAndRank([...linked, ...temporal]);\n  }\n}\n```\n\n#### Context Preservation Strategies\n\n1. **Session-Based Context**\n   - Store conversation history in session\n   - Maintain document focus across queries\n   - Track user\'s analysis path\n\n2. **Document-Based Context**\n   - Link related documents explicitly\n   - Use temporal proximity for implicit links\n   - Maintain document version history\n\n3. **Semantic Context**\n   - Store document embeddings for similarity search\n   - Enable "find similar results" functionality\n   - Support natural language document discovery\n\n4. **Historical Context**\n   - Track trends over time\n   - Compare current vs. previous results\n   - Identify patterns and anomalies\n\n### 5. OCR Processing Pipeline\n\n```typescript\nclass OCRPipeline {\n  async processDocument(documentId: string): Promise<ProcessingResult> {\n    try {\n      // 1. Extract raw text\n      const rawText = await this.extractText(documentId);\n      \n      // 2. Identify document type\n      const documentType = await this.classifyDocument(rawText);\n      \n      // 3. Parse structured data based on type\n      const structuredData = await this.parseByType(rawText, documentType);\n      \n      // 4. Extract key entities\n      const entities = await this.extractEntities(rawText);\n      \n      // 5. Generate embeddings for semantic search\n      const embeddings = await this.generateEmbeddings(rawText);\n      \n      // 6. Store results\n      await this.storeResults({\n        documentId,\n        rawText,\n        documentType,\n        structuredData,\n        entities,\n        embeddings\n      });\n      \n      return { success: true, documentType, dataPoints: structuredData.length };\n    } catch (error) {\n      await this.handleError(documentId, error);\n      return { success: false, error: error.message };\n    }\n  }\n  \n  async parseByType(text: string, type: DocumentType): Promise<StructuredData[]> {\n    switch (type) {\n      case \'bloodwork\':\n        return this.parseBloodwork(text);\n      case \'imaging\':\n        return this.parseImagingReport(text);\n      case \'aftercare\':\n        return this.parseAftercareSummary(text);\n      default:\n        return this.parseGeneric(text);\n    }\n  }\n  \n  async parseBloodwork(text: string): Promise<TestResult[]> {\n    // Extract test names, values, units, reference ranges\n    // Example patterns:\n    // "Hemoglobin: 14.5 g/dL (13.5-17.5)"\n    // "WBC: 7.2 K/uL (4.5-11.0)"\n    \n    const results: TestResult[] = [];\n    const lines = text.split(\'\\n\');\n    \n    for (const line of lines) {\n      const match = line.match(\n        /([A-Za-z0-9\\s]+):\\s*([\\d.]+)\\s*([A-Za-z/%]+)?\\s*\\(?([\\d.-]+)?\\s*-?\\s*([\\d.]+)?\\)?/\n      );\n      \n      if (match) {\n        results.push({\n          testName: match[1].trim(),\n          value: parseFloat(match[2]),\n          unit: match[3] || \'\',\n          referenceMin: match[4] ? parseFloat(match[4]) : null,\n          referenceMax: match[5] ? parseFloat(match[5]) : null,\n          isAbnormal: this.checkAbnormal(\n            parseFloat(match[2]),\n            match[4] ? parseFloat(match[4]) : null,\n            match[5] ? parseFloat(match[5]) : null\n          )\n        });\n      }\n    }\n    \n    return results;\n  }\n}\n```\n\n### 6. AI Integration Architecture\n\n```typescript\nclass AIAnalysisService {\n  private openai: OpenAI;\n  private contextBuilder: ContextBuilder;\n  private vectorStore: VectorStore;\n  \n  async analyzeWithContext(\n    query: string,\n    documentId: string,\n    userId: string\n  ): Promise<AIResponse> {\n    // 1. Build comprehensive context\n    const context = await this.contextBuilder.buildAnalysisContext(\n      documentId,\n      userId\n    );\n    \n    // 2. Retrieve relevant historical information\n    const relevantHistory = await this.vectorStore.similaritySearch(\n      query,\n      context.metadata.totalDocuments,\n      5\n    );\n    \n    // 3. Construct prompt with context\n    const prompt = this.buildPrompt(query, context, relevantHistory);\n    \n    // 4. Get AI response\n    const response = await this.openai.chat.completions.create({\n      model: "gpt-4-turbo-preview",\n      messages: [\n        {\n          role: "system",\n          content: this.getSystemPrompt()\n        },\n        {\n          role: "user",\n          content: prompt\n        }\n      ],\n      temperature: 0.3, // Lower temperature for medical accuracy\n    });\n    \n    // 5. Store interaction for future context\n    await this.storeInteraction(userId, query, response, context);\n    \n    return {\n      answer: response.choices[0].message.content,\n      sources: this.extractSources(context),\n      confidence: this.assessConfidence(response),\n      relatedDocuments: context.relatedDocuments.map(d => d.id)\n    };\n  }\n  \n  private buildPrompt(\n    query: string,\n    context: AnalysisContext,\n    history: Document[]\n  ): string {\n    return `\nYou are a medical document analysis assistant. Analyze the following query in the context of the provided medical documents.\n\nCURRENT DOCUMENT:\nType: ${context.currentDocument.documentType}\nDate: ${context.currentDocument.documentDate}\nExtracted Data: ${JSON.stringify(context.currentData, null, 2)}\n\nRELATED DOCUMENTS:\n${context.relatedDocuments.map(doc => `\n- ${doc.documentType} from ${doc.documentDate}\n  Key findings: ${this.summarizeDocument(doc)}\n`).join(\'\\n\')}\n\nHISTORICAL TRENDS:\n${this.formatHistoricalTrends(context.historicalTrends)}\n\nUSER QUERY:\n${query}\n\nPlease provide a comprehensive analysis that:\n1. Directly answers the user\'s question\n2. References specific data points from the documents\n3. Highlights any trends or patterns\n4. Notes any concerning findings\n5. Suggests follow-up questions or actions if appropriate\n\nRemember: This is for informational purposes only and should not replace professional medical advice.\n    `;\n  }\n  \n  private getSystemPrompt(): string {\n    return `\nYou are an AI assistant specialized in analyzing medical documents. Your role is to:\n\n1. Extract and interpret information from medical documents accurately\n2. Identify trends and patterns across multiple documents\n3. Provide clear, understandable explanations of medical data\n4. Cross-reference related documents to provide comprehensive context\n5. Highlight abnormal values or concerning findings\n6. Maintain patient privacy and data security\n\nGuidelines:\n- Always cite specific documents and data points in your responses\n- Use clear, non-technical language when possible\n- Indicate when values are outside normal ranges\n- Suggest when professional medical consultation is needed\n- Never provide definitive diagnoses or treatment recommendations\n- Always include appropriate medical disclaimers\n    `;\n  }\n}\n```\n\n## Implementation Priorities\n\n### Phase 1: MVP (Weeks 1-3)\n1. Basic document upload and storage\n2. Simple OCR text extraction\n3. Document viewer\n4. Basic user authentication\n5. Simple AI query interface\n\n### Phase 2: Intelligence (Weeks 4-6)\n1. Structured data extraction\n2. Document type classification\n3. Context-aware AI responses\n4. Document linking\n5. Basic trend visualization\n\n### Phase 3: Advanced Features (Weeks 7-10)\n1. Advanced trend analysis\n2. Predictive insights\n3. Multi-document comparison\n4. Enhanced context management\n5. Semantic search\n\n## Security Considerations\n\n1. **Data Encryption**\n   - Encrypt files at rest (AES-256)\n   - Use HTTPS for all communications\n   - Encrypt sensitive database fields\n\n2. **Access Control**\n   - Role-based access control (RBAC)\n   - Document-level permissions\n   - Audit logging for all access\n\n3. **HIPAA Compliance**\n   - Business Associate Agreements (BAA)\n   - Secure data transmission\n   - Data retention policies\n   - Breach notification procedures\n\n4. **API Security**\n   - JWT authentication\n   - Rate limiting\n   - Input validation\n   - SQL injection prevention\n</create-file>\n</function_calls>'}